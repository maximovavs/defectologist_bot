from __future__ import annotations

import os, re, json, time, random, hashlib, math
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlparse, urljoin

import requests, yaml, feedparser
from bs4 import BeautifulSoup
from dateutil import tz
from PIL import Image, ImageDraw, ImageFont, ImageFilter

ROOT = Path(__file__).resolve().parents[1]
CFG_DIR = ROOT / "config"
STATE_DIR = ROOT / ".state"
ASSETS_DIR = ROOT / "assets"
FONTS_DIR = ASSETS_DIR / "fonts"
STATE_DIR.mkdir(exist_ok=True)

USER_AGENT = "logoped-channel-bot/1.6 (+https://github.com/)"
HEADERS = {"User-Agent": USER_AGENT}

TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN","").strip()
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID","").strip()
TELEGRAM_DRAFTS_CHAT_ID = os.getenv("TELEGRAM_DRAFTS_CHAT_ID","").strip()

REWRITE_PROVIDER = os.getenv("REWRITE_PROVIDER","auto").strip().lower()  # none|auto|groq|gemini
GROQ_API_KEY = os.getenv("GROQ_API_KEY","").strip()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY","").strip()

AUDIENCE = os.getenv("AUDIENCE","parents").strip().lower()  # parents|pros|both

@dataclass
class Source:
    id: str
    name: str
    type: str
    url: Optional[str] = None
    urls: Optional[List[str]] = None
    parser: Optional[str] = None
    notes: str = ""

def load_yaml(path: Path) -> Dict[str,Any]:
    return yaml.safe_load(path.read_text(encoding="utf-8"))

def norm_space(s: str) -> str:
    return re.sub(r"\s+"," ",(s or "").strip())

def norm_title_key(s: str) -> str:
    s = (s or "").lower()
    s = re.sub(r"[^\w\s]+"," ",s,flags=re.UNICODE)
    s = re.sub(r"\s+"," ",s).strip()
    s = re.sub(r"\b(–ª–æ–≥–æ–ø–µ–¥|–ª–æ–≥–æ–ø–µ–¥–∏—è|–ª–æ–≥–æ–ø–µ–¥–∏—á–µ—Å–∫–∏–π|—É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ|—É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è)\b","",s).strip()
    s = re.sub(r"\s+"," ",s).strip()
    return s[:180]

def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8")).hexdigest()

def get_local_now(tzname: str) -> datetime:
    return datetime.now(tz=tz.gettz(tzname))

def iso_week_key(dt: datetime) -> str:
    y, w, _ = dt.isocalendar()
    return f"{y}-W{w:02d}"

def is_due(rubric: Dict[str,Any], now: datetime) -> bool:
    cadence = (rubric.get("cadence") or "DAILY").upper()
    if cadence == "DAILY":
        return True
    if cadence == "WEEKLY":
        byweekday = set(rubric.get("byweekday") or [])
        map_wd = ["MO","TU","WE","TH","FR","SA","SU"]
        return map_wd[now.weekday()] in byweekday
    return False

def load_sources() -> Tuple[Dict[str,Source], Dict[str,Any]]:
    cfg = load_yaml(CFG_DIR/"sources.yml")
    quality = cfg.get("quality",{})
    out: Dict[str,Source] = {}
    for s in cfg.get("sources",[]):
        out[s["id"]] = Source(**s)
    return out, quality

def load_state(name: str, default: Any) -> Any:
    p = STATE_DIR/name
    if not p.exists():
        return default
    try:
        return json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        return default

def save_state(name: str, data: Any) -> None:
    (STATE_DIR/name).write_text(json.dumps(data,ensure_ascii=False,indent=2),encoding="utf-8")

def safe_domain(url: str) -> str:
    try:
        return (urlparse(url).netloc or "").lower()
    except Exception:
        return ""

def domain_allowed(url: str, allow_domains: List[str]) -> bool:
    d = safe_domain(url)
    return bool(d) and any(d==ad or d.endswith("."+ad) for ad in allow_domains)

def score_item(title: str, link: str, quality_cfg: Dict[str,Any]) -> Tuple[int,str]:
    t = (title or "").strip()
    u = (link or "").strip()
    if len(t) < 12 or len(t) > 240:
        return (-100,"bad_title_len")
    allow_domains = quality_cfg.get("allow_domains") or []
    if allow_domains and not domain_allowed(u, allow_domains):
        return (-100,"domain_not_allowed")
    tl, ul = t.lower(), u.lower()
    for k in [x.lower() for x in (quality_cfg.get("deny_keywords") or [])]:
        if k and (k in tl or k in ul):
            return (-100,f"deny_keyword:{k}")
    score = 10
    for k in [x.lower() for x in (quality_cfg.get("boost_keywords") or [])]:
        if k and k in tl:
            score += 2
    return (score,"ok")

def get_canonical_and_soup(url: str) -> Tuple[str, Optional[BeautifulSoup]]:
    try:
        r = requests.get(url, headers=HEADERS, timeout=25)
        r.raise_for_status()
        soup = BeautifulSoup(r.text,"lxml")
        canon = soup.find("link", rel=lambda x: x and "canonical" in x.lower())
        if canon and canon.get("href"):
            href = canon["href"].strip()
            if href.startswith("/"):
                href = urljoin(url, href)
            return href, soup
        return url, soup
    except Exception:
        return url, None

def extract_article_title(soup: BeautifulSoup) -> str:
    og = soup.find("meta", property="og:title")
    if og and og.get("content"):
        return norm_space(og["content"])
    h1 = soup.find("h1")
    if h1:
        return norm_space(h1.get_text(" ",strip=True))
    if soup.title and soup.title.string:
        return norm_space(soup.title.string)
    return ""

def extract_article_summary(soup: BeautifulSoup) -> str:
    md = soup.find("meta", attrs={"name":"description"})
    if md and md.get("content"):
        return norm_space(md["content"])
    ogd = soup.find("meta", property="og:description")
    if ogd and ogd.get("content"):
        return norm_space(ogd["content"])
    paras=[]
    for p in soup.select("p"):
        txt = norm_space(p.get_text(" ",strip=True))
        if len(txt) < 60:
            continue
        if any(bad in txt.lower() for bad in ["cookie","–ø–æ–ª–∏—Ç–∏–∫","–ø–æ–¥–ø–∏—Å","—Ä–µ–∫–ª–∞–º–∞"]):
            continue
        paras.append(txt)
        if len(paras)>=2:
            break
    return norm_space(" ".join(paras))[:420]

def is_scientific_or_methodical(domain: str, title: str, summary: str, quality_cfg: Dict[str,Any]) -> Tuple[bool,str]:
    scientific_domains = [d.lower() for d in (quality_cfg.get("scientific_domains") or [])]
    if any(domain==d or domain.endswith("."+d) for d in scientific_domains):
        return True,"scientific_domain"
    blob = f"{title}\n{summary}".lower()
    kws = [k.lower() for k in (quality_cfg.get("methodical_keywords") or [])]
    hits = sum(1 for k in kws if k and k in blob)
    if hits >= 2:
        return True,f"methodical_kw_hits:{hits}"
    return False,f"not_methodical_hits:{hits}"

def source_type_label_from_factcheck(factcheck_reason: str) -> str:
    r = (factcheck_reason or "").lower()
    if "scientific_domain" in r:
        return "–Ω–∞—É—á–Ω—ã–π/–∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫"
    return "–º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏–π/–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª"

# ---------------------------
# Site-specific parsers (v1.4)
# ---------------------------

def _abs(url: str, href: str) -> str:
    href = (href or "").strip()
    if not href:
        return ""
    if href.startswith("//"):
        return "https:" + href
    if href.startswith("/"):
        return urljoin(url, href)
    if href.startswith("http://") or href.startswith("https://"):
        return href
    return urljoin(url, href)

def _collect_links(base_url: str, soup: BeautifulSoup, selector: str, href_re: Optional[str]=None) -> List[Dict[str,str]]:
    pat = re.compile(href_re) if href_re else None
    out=[]
    for a in soup.select(selector):
        href = _abs(base_url, a.get("href",""))
        if not href:
            continue
        if pat and not pat.search(href):
            continue
        title = norm_space(a.get_text(" ", strip=True))
        if not title or len(title) < 8:
            continue
        out.append({"title": title, "link": href, "summary": ""})
    seen=set(); uniq=[]
    for it in out:
        if it["link"] in seen:
            continue
        seen.add(it["link"])
        uniq.append(it)
    return uniq

def parse_logopediya_publ(url: str, html: str) -> List[Dict[str,str]]:
    soup = BeautifulSoup(html, "lxml")
    items = _collect_links(url, soup, "div#dle-content a, div#dle-content h2 a, div#dle-content h3 a", r"/publ/[^\"']+")
    items = [it for it in items if not re.search(r"/page/\d+/?$", it["link"])]
    return items[:80]

def parse_logorina_news(url: str, html: str) -> List[Dict[str,str]]:
    soup = BeautifulSoup(html, "lxml")
    items = _collect_links(url, soup, "article a, div.news a, a", r"/news/[\w\-]+/?$")
    return items[:80]

def parse_logomag_lib(url: str, html: str) -> List[Dict[str,str]]:
    soup = BeautifulSoup(html, "lxml")
    items = _collect_links(url, soup, "main a, div.content a, a", r"/lib/[^\"']+")
    return items[:80]

def parse_logoportal_articles(url: str, html: str) -> List[Dict[str,str]]:
    soup = BeautifulSoup(html, "lxml")
    items = _collect_links(url, soup, "main a, div#content a, article a, a", r"(statya-|/statya-)")
    return items[:80]

def parse_logopedy_articles(url: str, html: str) -> List[Dict[str,str]]:
    soup = BeautifulSoup(html, "lxml")
    items = _collect_links(url, soup, "div.content a, main a, a", r"logoped-article|logoped-literature|portal/[^#]+")
    items.sort(key=lambda x: len(x["title"]), reverse=True)
    return items[:80]

SITE_PARSERS = {
    "logopediya_publ": parse_logopediya_publ,
    "logorina_news": parse_logorina_news,
    "logomag_lib": parse_logomag_lib,
    "logoportal_articles": parse_logoportal_articles,
    "logopedy_articles": parse_logopedy_articles,
}

def fetch_rss(url: str) -> List[Dict[str,str]]:
    d = feedparser.parse(url)
    out=[]
    for e in d.entries[:50]:
        out.append({
            "title": norm_space(getattr(e,"title","")),
            "link": getattr(e,"link",""),
            "summary": norm_space(re.sub("<.*?>","",getattr(e,"summary",""))),
        })
    return out

def fetch_static(urls: List[str]) -> List[Dict[str,str]]:
    return [{"title":"","link":u,"summary":""} for u in urls]

def fetch_html_site(url: str, parser_name: str) -> List[Dict[str,str]]:
    r = requests.get(url, headers=HEADERS, timeout=30)
    r.raise_for_status()
    parser = SITE_PARSERS.get(parser_name)
    if not parser:
        raise ValueError(f"Unknown site parser: {parser_name}")
    items = parser(url, r.text)
    uniq={}
    for it in items:
        uniq[it["link"]] = it
    return list(uniq.values())

def fetch_source(src: Source) -> List[Dict[str,str]]:
    if src.type=="rss":
        return fetch_rss(src.url or "")
    if src.type=="html_site":
        return fetch_html_site(src.url or "", src.parser or "")
    if src.type=="static":
        return fetch_static(src.urls or [])
    raise ValueError(f"Unsupported source type: {src.type}")

def enrich_article(item: Dict[str,str]) -> Dict[str,str]:
    link = item.get("link","")
    canon, soup = get_canonical_and_soup(link)
    item["canonical"]=canon
    if soup:
        at = extract_article_title(soup)
        if at: item["article_title"]=at
        sm = extract_article_summary(soup)
        if sm: item["article_summary"]=sm
    return item

def _is_quota_error(status: int, text: str) -> bool:
    t=(text or "").lower()
    return status in (402,429) or any(k in t for k in ["quota","rate limit","exceeded","insufficient_quota"])

def rewrite_with_groq(prompt: str) -> str:
    if not GROQ_API_KEY: raise RuntimeError("GROQ_API_KEY missing")
    r = requests.post(
        "https://api.groq.com/openai/v1/chat/completions",
        headers={"Authorization": f"Bearer {GROQ_API_KEY}", "Content-Type":"application/json"},
        json={"model":"llama-3.1-8b-instant","messages":[{"role":"user","content":prompt}],"temperature":0.4},
        timeout=45
    )
    if r.status_code!=200 and _is_quota_error(r.status_code,r.text):
        raise RuntimeError(f"groq_quota:{r.status_code}")
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"].strip()

def rewrite_with_gemini(prompt: str) -> str:
    if not GEMINI_API_KEY: raise RuntimeError("GEMINI_API_KEY missing")
    r = requests.post(
        "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent",
        params={"key": GEMINI_API_KEY},
        json={"contents":[{"parts":[{"text":prompt}]}]},
        timeout=45
    )
    if r.status_code!=200 and _is_quota_error(r.status_code,r.text):
        raise RuntimeError(f"gemini_quota:{r.status_code}")
    r.raise_for_status()
    return r.json()["candidates"][0]["content"]["parts"][0]["text"].strip()

def rewrite_if_enabled(text: str) -> str:
    if REWRITE_PROVIDER=="none":
        return text

    # v1.6: protect Source + disclaimer + hashtags from rewriting
    marker = "\n**–ò—Å—Ç–æ—á–Ω–∏–∫**\n"
    idx = text.find(marker)

    if idx != -1:
        body = text[:idx].strip()
        tail = text[idx:].strip()
    else:
        # legacy fallback (older template)
        parts = text.split("–ò—Å—Ç–æ—á–Ω–∏–∫:",1)
        body = parts[0].strip()
        tail = ("–ò—Å—Ç–æ—á–Ω–∏–∫:"+parts[1]).strip() if len(parts)==2 else ""

    prompt = (
        "–ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —Ç–µ–∫—Å—Ç –Ω–∏–∂–µ –ø–æ-—Ä—É—Å—Å–∫–∏: —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π, –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ-–Ω–∞—É—á–Ω—ã–π, –±–µ–∑ –¥–∏–∞–≥–Ω–æ–∑–æ–≤ –∏ –æ–±–µ—â–∞–Ω–∏–π –ª–µ—á–µ–Ω–∏—è. "
        "–ù–µ –¥–æ–±–∞–≤–ª—è–π —Ñ–∞–∫—Ç—ã. –°–æ—Ö—Ä–∞–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –ø–æ—Ä—è–¥–æ–∫ –±–ª–æ–∫–æ–≤, –∞ —Ç–∞–∫–∂–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å–µ–∫—Ü–∏–π –∏ —Å–ø–∏—Å–∫–∏. "
        "–ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å–µ–∫—Ü–∏–π: ¬´–°—É—Ç—å¬ª, ¬´–ß—Ç–æ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç –¥–ª—è –≤–∞—Å¬ª, ¬´–ü—Ä–∞–∫—Ç–∏–∫–∞ –Ω–∞ —Å–µ–≥–æ–¥–Ω—è (5‚Äì7 –º–∏–Ω—É—Ç)¬ª, ¬´–ù–æ—Ä–º–∞ / –∫–æ–≥–¥–∞ –Ω—É–∂–µ–Ω —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç¬ª. "
        "–°–æ—Ö—Ä–∞–Ω–∏ —ç–º–æ–¥–∑–∏. –ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–æ–≤—ã–µ —Ä–∞–∑–¥–µ–ª—ã.\n\n"
        "–¢–ï–ö–°–¢:\n"+body+"\n"
    )

    try:
        if REWRITE_PROVIDER in ("groq","auto"):
            try:
                out = rewrite_with_groq(prompt)
                return out.strip() + ("\n\n"+tail if tail else "")
            except Exception as e:
                if REWRITE_PROVIDER=="groq":
                    raise
                if "groq_quota" in str(e):
                    print("[WARN] groq quota; fallback to gemini")
                else:
                    print(f"[WARN] groq rewrite failed: {e}")
        if REWRITE_PROVIDER in ("gemini","auto"):
            out = rewrite_with_gemini(prompt)
            return out.strip() + ("\n\n"+tail if tail else "")
    except Exception as e:
        print(f"[WARN] rewrite failed ({REWRITE_PROVIDER}): {e}")
        return text

    return text

def make_question_week() -> str:
    questions = [
        "–†–µ–±—ë–Ω–æ–∫ –ø–æ–Ω–∏–º–∞–µ—Ç –æ–±—Ä–∞—â—ë–Ω–Ω—É—é —Ä–µ—á—å, –Ω–æ –≥–æ–≤–æ—Ä–∏—Ç –º–∞–ª–æ: –∫–∞–∫–∏–µ —à–∞–≥–∏ –≤—ã —É–∂–µ –ø—Ä–æ–±–æ–≤–∞–ª–∏ –¥–æ–º–∞?",
        "–í –±–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–æ–π —Å–µ–º—å–µ: –Ω–∞ –∫–∞–∫–æ–º —è–∑—ã–∫–µ —Ä–µ–±—ë–Ω–∫—É –ª–µ–≥—á–µ —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞—Ç—å –∏—Å—Ç–æ—Ä–∏–∏ –∏ –ø–æ—á–µ–º—É?",
        "–ö–∞–∫–∏–µ –∑–≤—É–∫–∏/—Å–ª–æ–≥–∏ –¥–∞—é—Ç—Å—è —Ç—Ä—É–¥–Ω–µ–µ –≤—Å–µ–≥–æ ‚Äî –∏ –≤ –∫–∞–∫–∏—Ö —Å–ª–æ–≤–∞—Ö —ç—Ç–æ –∑–∞–º–µ—Ç–Ω–µ–µ?",
        "–ß—Ç–æ –≤—ã–∑—ã–≤–∞–µ—Ç –±–æ–ª—å—à–µ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è: –∞—Ä—Ç–∏–∫—É–ª—è—Ü–∏–æ–Ω–Ω–∞—è –≥–∏–º–Ω–∞—Å—Ç–∏–∫–∞, –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ —Å–ª–æ–≥–æ–≤ –∏–ª–∏ —á—Ç–µ–Ω–∏–µ/–ø–∏—Å—å–º–æ?",
        "–ö–∞–∫ –≤—ã–≥–ª—è–¥–∏—Ç –≤–∞—à ¬´–∏–¥–µ–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç¬ª —á–µ—Ä–µ–∑ 4 –Ω–µ–¥–µ–ª–∏ –∑–∞–Ω—è—Ç–∏–π ‚Äî –≤ –æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏?",
    ]
    return random.choice(questions)

def _bullets(lines: List[str]) -> str:
    clean = [norm_space(x) for x in lines if norm_space(x)]
    return "\n".join([f"‚Ä¢ {x}" for x in clean])

def _numbered(lines: List[str]) -> str:
    clean = [norm_space(x) for x in lines if norm_space(x)]
    return "\n".join([f"{i+1}) {x}" for i,x in enumerate(clean)])

def build_post_v2(
    rubric_title: str,
    rubric_format: str,
    audience: str,
    channel_cfg: Dict[str,Any],
    picked: Dict[str,str],
    title_suffix: str
) -> str:
    """
    v1.6 ‚Äî post_template_v2 with mandatory blocks for all rubrics (except quality_dashboard).
    Blocks:
      - –°—É—Ç—å
      - –ß—Ç–æ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç –¥–ª—è –≤–∞—Å
      - –ü—Ä–∞–∫—Ç–∏–∫–∞ –Ω–∞ —Å–µ–≥–æ–¥–Ω—è (5‚Äì7 –º–∏–Ω—É—Ç)
      - –ù–æ—Ä–º–∞ / –∫–æ–≥–¥–∞ –Ω—É–∂–µ–Ω —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç
      - –ò—Å—Ç–æ—á–Ω–∏–∫ (link + type)
    """
    link = picked.get("canonical") or picked.get("link","")
    picked_title = picked.get("picked_title") or picked.get("title") or ""
    summary = picked.get("picked_summary") or picked.get("summary") or ""
    disclaimer = channel_cfg.get("disclaimer","")
    tags = " ".join(channel_cfg.get("hashtags",[]))

    aud = (audience or "parents").strip().lower()
    rf = (rubric_format or "").strip().lower()

    # --- –°—É—Ç—å
    if rf == "question_week":
        q = make_question_week()
        essence = (
            "–ù–µ–±–æ–ª—å—à–æ–π ‚Äú–≤–æ–ø—Ä–æ—Å –Ω–µ–¥–µ–ª–∏‚Äù ‚Äî —á—Ç–æ–±—ã –º—è–≥–∫–æ –ø–æ–Ω—è—Ç—å —Ç–µ–∫—É—â—É—é —Å–∏—Ç—É–∞—Ü–∏—é –∏ –≤—ã–±—Ä–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —à–∞–≥.\n\n"
            f"**{q}**"
        )
        if not picked_title:
            picked_title = "–†—É–±—Ä–∏–∫–∞ –∫–∞–Ω–∞–ª–∞ (–≤–æ–ø—Ä–æ—Å –¥–ª—è —Å–∞–º–æ–Ω–∞–±–ª—é–¥–µ–Ω–∏—è)"
        if not summary:
            summary = "–§–æ—Ä–º–∞—Ç: –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ, –º–∞–ª–µ–Ω—å–∫–∏–π —à–∞–≥, –±–µ–∑ –¥–∞–≤–ª–µ–Ω–∏—è."
    else:
        essence_lines = []
        if picked_title:
            essence_lines.append(f"–ú–∞—Ç–µ—Ä–∏–∞–ª: {picked_title}")
        if summary:
            essence_lines.append(f"–ö–æ—Ä–æ—Ç–∫–æ: {summary}")
        essence = "\n".join(essence_lines).strip() or "–ö–æ—Ä–æ—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É –æ —Ä–∞–∑–≤–∏—Ç–∏–∏ —Ä–µ—á–∏."

    # --- –ß—Ç–æ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç –¥–ª—è –≤–∞—Å (2‚Äì3 –ø—É–Ω–∫—Ç–∞)
    meaning: List[str]
    if rf == "bilingual_parents":
        meaning = [
            "–°–º–µ—à–∏–≤–∞–Ω–∏–µ —è–∑—ã–∫–æ–≤ –∏ ‚Äú–≤—Å—Ç–∞–≤–∫–∏‚Äù —Å–ª–æ–≤ –≤—Ç–æ—Ä–æ–≥–æ —è–∑—ã–∫–∞ —á–∞—Å—Ç–æ –±—ã–≤–∞—é—Ç —á–∞—Å—Ç—å—é –Ω–æ—Ä–º—ã –≤ –±–∏–ª–∏–Ω–≥–≤–∏–∑–º–µ.",
            "–ó–∞–ø—Ä–µ—Ç—ã –∏ –¥–∞–≤–ª–µ–Ω–∏–µ –æ–±—ã—á–Ω–æ —Å–Ω–∏–∂–∞—é—Ç –º–æ—Ç–∏–≤–∞—Ü–∏—é –≥–æ–≤–æ—Ä–∏—Ç—å ‚Äî –ª—É—á—à–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —Ä—É—Å—Å–∫–∏–π —Ä–µ–≥—É–ª—è—Ä–Ω–æ –∏ —Å–ø–æ–∫–æ–π–Ω–æ.",
            "–í–∞–∂–Ω–µ–µ —Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏ –¥–∏–Ω–∞–º–∏–∫—É, –∞ –Ω–µ –Ω–∞ –∏–¥–µ–∞–ª—å–Ω—É—é ‚Äú—á–∏—Å—Ç–æ—Ç—É‚Äù —è–∑—ã–∫–∞ –≤ –∫–∞–∂–¥—ã–π –º–æ–º–µ–Ω—Ç.",
        ]
    elif rf == "exercise_steps":
        meaning = [
            "–ö–æ—Ä–æ—Ç–∫–∞—è —Ä–µ–≥—É–ª—è—Ä–Ω–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —Ä–µ–¥–∫–∏—Ö ‚Äú–¥–ª–∏–Ω–Ω—ã—Ö‚Äù –∑–∞–Ω—è—Ç–∏–π.",
            "–ó–µ—Ä–∫–∞–ª–æ –∏ –∏–≥—Ä–∞ –ø–æ–º–æ–≥–∞—é—Ç —É–¥–µ—Ä–∂–∞—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –∏ —Å–¥–µ–ª–∞—Ç—å —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ –ø—Ä–∏–≤—ã—á–∫–æ–π.",
            "–ï—Å–ª–∏ —Ä–µ–±—ë–Ω–æ–∫ —É—Å—Ç–∞–ª ‚Äî –ª—É—á—à–µ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è —Ä–∞–Ω—å—à–µ, —á–µ–º –∑–∞–∫—Ä–µ–ø–∏—Ç—å —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ.",
        ]
    elif rf == "myth_fact":
        meaning = [
            "–ü–æ–ª–µ–∑–Ω–æ –æ—Ç–¥–µ–ª—è—Ç—å –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–∏—Ñ—ã –æ—Ç —Ç–æ–≥–æ, —á—Ç–æ —Ä–µ–∞–ª—å–Ω–æ –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –≤ —Ä–∞–∑–≤–∏—Ç–∏–∏ —Ä–µ—á–∏.",
            "–û–±—ã—á–Ω–æ –≤–∞–∂–Ω–µ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ, –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è –∏ –¥–∏–Ω–∞–º–∏–∫–∞, —á–µ–º –µ–¥–∏–Ω–∏—á–Ω—ã–µ ‚Äú—Å–∏–º–ø—Ç–æ–º—ã‚Äù.",
            "–ï—Å–ª–∏ —Ç—Ä–µ–≤–æ–∂–Ω–æ ‚Äî –ª—É—á—à–µ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Å–∏—Ç—É–∞—Ü–∏—é –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ, –∞ –Ω–µ –ø–æ –æ–¥–Ω–æ–º—É –ø—Ä–∏–∑–Ω–∞–∫—É.",
        ]
    elif rf == "age_norms":
        meaning = [
            "–í–æ–∑—Ä–∞—Å—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã ‚Äî –æ—Ä–∏–µ–Ω—Ç–∏—Ä, –∞ –Ω–µ ‚Äú—ç–∫–∑–∞–º–µ–Ω‚Äù: –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–æ—Ä–º—ã –±—ã–≤–∞—é—Ç —à–∏—Ä–æ–∫–∏–º–∏.",
            "–ì–ª–∞–≤–Ω–æ–µ ‚Äî –¥–∏–Ω–∞–º–∏–∫–∞: —Ä–∞—Å—Ç—ë—Ç –ª–∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏ –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞ –æ–±—â–µ–Ω–∏—è, –ø–æ—è–≤–ª—è—é—Ç—Å—è –ª–∏ –Ω–æ–≤—ã–µ —Å–ª–æ–≤–∞/—Ñ—Ä–∞–∑—ã.",
            "–£–¥–æ–±–Ω–µ–µ –æ–±—Å—É–∂–¥–∞—Ç—å —Å–æ–º–Ω–µ–Ω–∏—è –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –ø—Ä–∏–º–µ—Ä–∞–º, –∞ –Ω–µ ‚Äú–ø–æ –æ—â—É—â–µ–Ω–∏—è–º‚Äù.",
        ]
    elif rf in ("pro_friendly","case_digest"):
        if aud == "parents":
            meaning = [
                "–ù–∏–∂–µ ‚Äî –∏–¥–µ—è, –∫–∞–∫ –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –º–∞—Ç–µ—Ä–∏–∞–ª –≤ –ø–æ–Ω—è—Ç–Ω—ã–π –¥–æ–º–∞—à–Ω–∏–π —à–∞–≥ –±–µ–∑ –ø–µ—Ä–µ–≥—Ä—É–∑–∞.",
                "–ï—Å–ª–∏ —Ä–µ–±—ë–Ω–∫—É —Å–ª–æ–∂–Ω–æ ‚Äî –Ω–∞—á–∏–Ω–∞–π—Ç–µ —Å –º–∞–ª–æ–≥–æ –∏ —Ñ–∏–∫—Å–∏—Ä—É–π—Ç–µ –º–∞–ª–µ–Ω—å–∫–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å.",
                "–°–∏—Å—Ç–µ–º–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ –∏–¥–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è.",
            ]
        else:
            meaning = [
                "–ü–µ—Ä–µ–Ω–æ—Å–∏—Ç–µ –º–∞—Ç–µ—Ä–∏–∞–ª –≤ –ø—Ä–∞–∫—Ç–∏–∫—É: —Ü–µ–ª—å ‚Üí –∫—Ä–∏—Ç–µ—Ä–∏–π ‚Üí —à–∞–≥–∏ ‚Üí –∫–æ–Ω—Ç—Ä–æ–ª—å.",
                "–î–ª—è –æ–Ω–ª–∞–π–Ω –æ—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω—ã –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è, –ø—Ä–æ—Å—Ç–æ–µ –î–ó –∏ –∫–æ—Ä–æ—Ç–∫–∏–π —á–µ–∫-–ª–∏—Å—Ç –¥–ª—è —Ä–æ–¥–∏—Ç–µ–ª–µ–π.",
                "–£—á–∏—Ç—ã–≤–∞–π—Ç–µ –±–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω—É—é —Å—Ä–µ–¥—É –∏ –ø–µ—Ä–µ–Ω–æ—Å –Ω–∞–≤—ã–∫–æ–≤ –º–µ–∂–¥—É —è–∑—ã–∫–∞–º–∏.",
            ]
    else:
        meaning = [
            "–°–∞–º—ã–π –Ω–∞–¥—ë–∂–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ —Ä–µ—á–∏ ‚Äî —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –º–∞–ª–µ–Ω—å–∫–∏–µ —à–∞–≥–∏, –∞ –Ω–µ —Ä–∞–∑–æ–≤—ã–µ ‚Äú—Ä—ã–≤–∫–∏‚Äù.",
            "–ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è –≤–∞–∂–Ω–µ–µ –∏–¥–µ–∞–ª—å–Ω–æ–π –∞—Ä—Ç–∏–∫—É–ª—è—Ü–∏–∏: —Å–Ω–∞—á–∞–ª–∞ —Å–º—ã—Å–ª –∏ –∂–µ–ª–∞–Ω–∏–µ –≥–æ–≤–æ—Ä–∏—Ç—å, –ø–æ—Ç–æ–º —Ç–æ—á–Ω–æ—Å—Ç—å.",
            "–õ—É—á—à–µ –æ–ø–∏—Ä–∞—Ç—å—Å—è –Ω–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ –Ω–∞–±–ª—é–¥–∞—Ç—å –¥–∏–Ω–∞–º–∏–∫—É 2‚Äì4 –Ω–µ–¥–µ–ª–∏.",
        ]

    # --- –ü—Ä–∞–∫—Ç–∏–∫–∞ 5‚Äì7 –º–∏–Ω—É—Ç
    practice: List[str]
    if rf == "exercise_steps":
        practice = [
            "–ü–µ—Ä–µ–¥ –∑–µ—Ä–∫–∞–ª–æ–º: ¬´–õ–æ–ø–∞—Ç–æ—á–∫–∞¬ª ‚Äî 5 —Ä–∞–∑ –ø–æ 5 —Å–µ–∫—É–Ω–¥.",
            "¬´–ß–∞—Å–∏–∫–∏¬ª ‚Äî 10 –ø–ª–∞–≤–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π –≤–ø—Ä–∞–≤–æ-–≤–ª–µ–≤–æ.",
            "1 –º–∏–Ω—É—Ç–∞: –¥—É–µ–º –Ω–∞ –≤–∞—Ç–Ω—ã–π —à–∞—Ä–∏–∫/–º—ã–ª—å–Ω—ã–µ –ø—É–∑—ã—Ä–∏ (–≤ –∏–≥—Ä–µ).",
            "–í –∫–æ–Ω—Ü–µ ‚Äî –∫–æ—Ä–æ—Ç–∫–∞—è –ø–æ—Ö–≤–∞–ª–∞ –∑–∞ –ø–æ–ø—ã—Ç–∫–∏, –±–µ–∑ ‚Äú–ø–µ—Ä–µ–¥–µ–ª—ã–≤–∞–π‚Äù.",
        ]
    elif rf == "bilingual_parents":
        practice = [
            "–ò–≥—Ä–∞ ‚Äú–î–≤–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞‚Äù: –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ —Ñ—Ä–∞–∑—É —Ä–µ–±—ë–Ω–∫–∞ –ø–æ-—Ä—É—Å—Å–∫–∏ (—Å–ø–æ–∫–æ–π–Ω–æ, –±–µ–∑ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π), –∑–∞—Ç–µ–º —Å–ø—Ä–æ—Å–∏—Ç–µ ¬´–∫–∞–∫ –ø–æ-—Ä—É—Å—Å–∫–∏?¬ª.",
            "5 –º–∏–Ω—É—Ç ‚Äú–æ—Å—Ç—Ä–æ–≤–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ‚Äù: –∫–Ω–∏–∂–∫–∞/–∫–∞—Ä—Ç–∏–Ω–∫–∏/–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π –¥–æ–º–∞.",
            "–í –∫–æ–Ω—Ü–µ –¥–Ω—è: —Ä–µ–±—ë–Ω–æ–∫ –≤—ã–±–∏—Ä–∞–µ—Ç 3 –ø—Ä–µ–¥–º–µ—Ç–∞ –∏ –Ω–∞–∑—ã–≤–∞–µ—Ç –∏—Ö –ø–æ-—Ä—É—Å—Å–∫–∏ (–º–æ–∂–Ω–æ —Å –ø–æ–¥—Å–∫–∞–∑–∫–æ–π).",
        ]
    elif rf == "question_week":
        practice = [
            "–ó–∞–ø–∏—à–∏—Ç–µ 3 –ø—Ä–∏–º–µ—Ä–∞ —Ñ—Ä–∞–∑ —Ä–µ–±—ë–Ω–∫–∞ (–∫–∞–∫ –µ—Å—Ç—å) –∏ —Å–∏—Ç—É–∞—Ü–∏–∏, –≥–¥–µ –æ–Ω–∏ –ø—Ä–æ–∑–≤—É—á–∞–ª–∏.",
            "–û—Ç–º–µ—Ç—å—Ç–µ: –ø–æ–Ω–∏–º–∞–µ—Ç –ª–∏ —Ä–µ–±—ë–Ω–æ–∫ –ø—Ä–æ—Å—å–±—ã –±–µ–∑ –∂–µ—Å—Ç–æ–≤ (2‚Äì3 –ø—Ä–∏–º–µ—Ä–∞).",
            "–í—ã–±–µ—Ä–∏—Ç–µ 1 –º–∏–Ω–∏-–∏–≥—Ä—É –Ω–∞ —Ä–µ—á—å –Ω–∞ 5 –º–∏–Ω—É—Ç (–∫–∞—Ä—Ç–∏–Ω–∫–∏/–æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–µ–¥–º–µ—Ç–∞/–ø—É–∑—ã—Ä–∏).",
        ]
    elif rf == "age_norms":
        practice = [
            "5 –º–∏–Ω—É—Ç ‚Äú–æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–æ–π —Ä–µ—á–∏‚Äù: –≤—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–µ–¥–º–µ—Ç –∏ –ø—Ä–æ–≥–æ–≤–æ—Ä–∏—Ç–µ (—Ü–≤–µ—Ç/—Ñ–æ—Ä–º–∞/–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ/–¥–µ–π—Å—Ç–≤–∏–µ).",
            "–ò–≥—Ä–∞ ‚Äú–ö—Ç–æ —á—Ç–æ –¥–µ–ª–∞–µ—Ç?‚Äù: 10 –≥–ª–∞–≥–æ–ª–æ–≤ –ø–æ –∫–∞—Ä—Ç–∏–Ω–∫–∞–º (–ø—Ä—ã–≥–∞–µ—Ç, —Ä–∏—Å—É–µ—Ç, –º–æ–µ—Ç‚Ä¶).",
            "–ï—Å–ª–∏ —Ä–µ–±—ë–Ω–æ–∫ –±–∏–ª–∏–Ω–≥–≤ ‚Äî –¥–∞–π—Ç–µ –æ—Ç–≤–µ—Ç–∏—Ç—å, –∑–∞—Ç–µ–º –º—è–≥–∫–æ –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –ø–æ-—Ä—É—Å—Å–∫–∏.",
        ]
    elif rf == "myth_fact":
        practice = [
            "–í—ã–±–µ—Ä–∏—Ç–µ 1 —Å–∏—Ç—É–∞—Ü–∏—é –¥–ª—è —Å–ø–æ–∫–æ–π–Ω–æ–≥–æ ‚Äú–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è‚Äù: –ø–æ–≤—Ç–æ—Ä–∏—Ç–µ —Ñ—Ä–∞–∑—É —Ä–µ–±—ë–Ω–∫–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ, –±–µ–∑ –æ—Ü–µ–Ω–∫–∏.",
            "5 –º–∏–Ω—É—Ç –∏–≥—Ä—ã –Ω–∞ —Å–ª–æ–≤–∞—Ä—å (–∫–∞—Ç–µ–≥–æ—Ä–∏–∏: –µ–¥–∞/–æ–¥–µ–∂–¥–∞/–∏–≥—Ä—É—à–∫–∏).",
            "–í –∫–æ–Ω—Ü–µ –∑–∞–¥–∞–π—Ç–µ –æ–¥–∏–Ω –æ—Ç–∫—Ä—ã—Ç—ã–π –≤–æ–ø—Ä–æ—Å: ¬´–ß—Ç–æ –±—ã–ª–æ —Å–∞–º—ã–º –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–º?¬ª",
        ]
    elif rf in ("pro_friendly","case_digest") and aud != "parents":
        practice = [
            "–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ —Ü–µ–ª—å –Ω–∞ 2 –Ω–µ–¥–µ–ª–∏ (1‚Äì2 –∏–∑–º–µ—Ä–∏–º—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏—è).",
            "–ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é 1 —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è + —á–µ–∫-–ª–∏—Å—Ç –¥–ª—è —Ä–æ–¥–∏—Ç–µ–ª–µ–π (–¥–æ 6 –ø—É–Ω–∫—Ç–æ–≤).",
            "–ü—Ä–æ–¥—É–º–∞–π—Ç–µ –ø–µ—Ä–µ–Ω–æ—Å –≤ –±–∏–ª–∏–Ω–≥–≤–∞–ª—å–Ω–æ–π —Å—Ä–µ–¥–µ: –≥–¥–µ –∑–∞–∫—Ä–µ–ø–ª—è—Ç—å —Ä—É—Å—Å–∫–∏–π –µ–∂–µ–¥–Ω–µ–≤–Ω–æ 5‚Äì10 –º–∏–Ω—É—Ç.",
        ]
    else:
        practice = [
            "5 –º–∏–Ω—É—Ç –∞—Ä—Ç–∏–∫—É–ª—è—Ü–∏–æ–Ω–Ω–æ–π –≥–∏–º–Ω–∞—Å—Ç–∏–∫–∏ (–≤ –∏–≥—Ä–µ, –ø–µ—Ä–µ–¥ –∑–µ—Ä–∫–∞–ª–æ–º).",
            "5 –º–∏–Ω—É—Ç ‚Äú—Å–ª–æ–≤–∞—Ä–Ω–æ–π –∏–≥—Ä—ã‚Äù: –∫–∞—Ç–µ–≥–æ—Ä–∏–∏/–ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–æ—Å—Ç–∏/–æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–µ–¥–º–µ—Ç–æ–≤.",
            "1 –º–∏–Ω—É—Ç–∞ –¥—ã—Ö–∞—Ç–µ–ª—å–Ω–æ–π –∏–≥—Ä—ã (–ø—É–∑—ã—Ä–∏/–≤–∞—Ç–Ω—ã–π —à–∞—Ä–∏–∫/–¥—É–µ–º –Ω–∞ –ø–µ—Ä—ã—à–∫–æ).",
        ]

    # --- –ù–æ—Ä–º–∞ vs –∫–æ–≥–¥–∞ –Ω—É–∂–µ–Ω —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç
    if rf in ("pro_friendly","case_digest") and aud != "parents":
        norm_lines = [
            "‚úÖ –ù–æ—Ä–º–∞: –µ—Å—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∫–æ–Ω—Ç–∞–∫—Ç, –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π, –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ –ø–æ —Ü–µ–ª—è–º.",
            "‚ö†Ô∏è –û–±—Å—É–¥–∏—Ç—å —Å–æ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–º: –≤—ã—Ä–∞–∂–µ–Ω–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è –Ω–∞–≤—ã–∫–æ–≤, —Å—Ç–æ–π–∫–∞—è —É—Å—Ç–∞–ª–æ—Å—Ç—å/–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ –ø—Ä–∏ –≥–æ–≤–æ—Ä–µ–Ω–∏–∏, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø—Ä–∏ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–µ 4‚Äì6 –Ω–µ–¥–µ–ª—å.",
        ]
    else:
        norm_lines = [
            "‚úÖ –ù–æ—Ä–º–∞: —Ä–µ–±—ë–Ω–æ–∫ –ø–æ–Ω–∏–º–∞–µ—Ç –æ–±—Ä–∞—â—ë–Ω–Ω—É—é —Ä–µ—á—å, –æ–±—â–∞–µ—Ç—Å—è (–∂–µ—Å—Ç–∞–º–∏/—Å–ª–æ–≤–∞–º–∏), –∏ –µ—Å—Ç—å –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ –Ω–µ–¥–µ–ª—è–º.",
            "‚ö†Ô∏è –û–±—Å—É–¥–∏—Ç—å —Å–æ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–º: –µ—Å–ª–∏ —Ä–µ–±—ë–Ω–æ–∫ —á–∞—Å—Ç–æ –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç –ø—Ä–æ—Å—Ç—ã–µ –ø—Ä–æ—Å—å–±—ã, —Ä–µ–∑–∫–æ ‚Äú—Ç–µ—Ä—è–µ—Ç‚Äù –Ω–∞–≤—ã–∫–∏, –∏–∑–±–µ–≥–∞–µ—Ç –æ–±—â–µ–Ω–∏—è –∏–ª–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –Ω–µ—Ç –ø—Ä–∏ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–µ 4‚Äì6 –Ω–µ–¥–µ–ª—å.",
        ]

    factcheck = picked.get("fact_check") or ""
    stype = picked.get("source_type") or source_type_label_from_factcheck(factcheck)

    parts: List[str] = []
    parts.append(f"**{rubric_title} {title_suffix}**")
    parts.append("")
    parts.append("**–°—É—Ç—å**")
    parts.append(essence)
    parts.append("")
    parts.append("**–ß—Ç–æ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç –¥–ª—è –≤–∞—Å**")
    parts.append(_bullets(meaning))
    parts.append("")
    parts.append("**–ü—Ä–∞–∫—Ç–∏–∫–∞ –Ω–∞ —Å–µ–≥–æ–¥–Ω—è (5‚Äì7 –º–∏–Ω—É—Ç)**")
    parts.append(_numbered(practice))
    parts.append("")
    parts.append("**–ù–æ—Ä–º–∞ / –∫–æ–≥–¥–∞ –Ω—É–∂–µ–Ω —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç**")
    parts.append("\n".join(norm_lines))
    parts.append("")
    parts.append("**–ò—Å—Ç–æ—á–Ω–∏–∫**")
    parts.append(f"üîó {link}" if link else "üîó (—Å—Å—ã–ª–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)")
    parts.append(f"–¢–∏–ø: {stype}")

    if disclaimer:
        parts.append("")
        parts.append(f"_{disclaimer}_")
    if tags:
        parts.append("")
        parts.append(tags)

    return rewrite_if_enabled("\n".join(parts).strip())

def make_text(
    rubric_title: str,
    rubric_format: str,
    audience: str,
    channel_cfg: Dict[str,Any],
    picked: Dict[str,str],
    title_suffix: str
) -> str:
    # v1.6 ‚Äî route through post_template_v2
    return build_post_v2(rubric_title, rubric_format, audience, channel_cfg, picked, title_suffix)

# ---------------------------
# Card rendering (v1.4)
# ---------------------------

def _load_font(size: int) -> ImageFont.FreeTypeFont:
    ttf = FONTS_DIR/"DejaVuSans.ttf"
    if ttf.exists():
        return ImageFont.truetype(str(ttf), size=size)
    return ImageFont.load_default()

def _hex_to_rgb(h: str) -> Tuple[int,int,int]:
    h = (h or "").strip().lstrip("#")
    if len(h)==3:
        h = "".join([c+c for c in h])
    if len(h)!=6:
        return (74,144,226)
    return tuple(int(h[i:i+2],16) for i in (0,2,4))

def render_image_card(rubric_title: str, subtitle: str, branding: Dict[str,Any]) -> Path:
    """
    Visual themes (switch in config/rubrics.yml -> branding.card_theme):
      - minimal: clean neutral, subtle waves
      - kids: softer palette, playful dots
      - scientific: stricter palette, grid accents
    """
    theme = (branding or {}).get("card_theme","minimal") or "minimal"
    theme = str(theme).strip().lower()

    W,H = 1280,720

    accent = _hex_to_rgb((branding or {}).get("card_accent","#4A90E2"))

    # Theme palettes
    if theme == "kids":
        bg_top = (252, 246, 255)
        bg_bottom = (240, 252, 255)
        panel_fill = (255,255,255)
        panel_outline = (236,230,244)
        title_color = (32, 36, 46)
        sub_color = (78, 86, 104)
        footer_color = (120, 126, 140)
        wave_alpha = 30
    elif theme == "scientific":
        bg_top = (245, 247, 250)
        bg_bottom = (232, 236, 244)
        panel_fill = (255,255,255)
        panel_outline = (220,226,235)
        title_color = (16, 20, 30)
        sub_color = (54, 62, 78)
        footer_color = (98, 104, 118)
        wave_alpha = 22
        # if accent too "bright", enforce deep blue-ish
        if sum(accent) > 560:
            accent = (36, 79, 166)
    else:  # minimal
        bg_top = (245, 247, 250)
        bg_bottom = (235, 240, 246)
        panel_fill = (255,255,255)
        panel_outline = (235,238,242)
        title_color = (24, 32, 44)
        sub_color = (70, 78, 92)
        footer_color = (110, 118, 132)
        wave_alpha = 26

    img = Image.new("RGB",(W,H),bg_top)
    draw = ImageDraw.Draw(img)

    # gradient background
    for y in range(H):
        t = y/(H-1)
        r = int(bg_top[0] + (bg_bottom[0]-bg_top[0])*t)
        g = int(bg_top[1] + (bg_bottom[1]-bg_top[1])*t)
        b = int(bg_top[2] + (bg_bottom[2]-bg_top[2])*t)
        draw.line([(0,y),(W,y)], fill=(r,g,b))

    # accents layer
    layer = Image.new("RGBA",(W,H),(0,0,0,0))
    ld = ImageDraw.Draw(layer)

    if theme in ("minimal","scientific"):
        # subtle wave strokes
        for i in range(3):
            y0 = 440 + i*55
            pts=[]
            for x in range(0,W+1,40):
                yy = y0 + int(12*math.sin((x/140.0) + i))
                pts.append((x,yy))
            ld.line(pts, fill=(*accent, wave_alpha), width=6 if theme=="minimal" else 5)

        if theme == "scientific":
            # faint grid in top-right
            gx0, gy0, gx1, gy1 = 760, 60, 1240, 300
            step = 34
            grid_col = (accent[0], accent[1], accent[2], 16)
            for x in range(gx0, gx1, step):
                ld.line([(x,gy0),(x,gy1)], fill=grid_col, width=2)
            for y in range(gy0, gy1, step):
                ld.line([(gx0,y),(gx1,y)], fill=grid_col, width=2)

    elif theme == "kids":
        # playful dots (deterministic per rubric)
        seed = int(hashlib.sha1((rubric_title or "").encode("utf-8")).hexdigest()[:8], 16)
        rng = random.Random(seed)
        dot_col = (accent[0], accent[1], accent[2], 22)
        for _ in range(120):
            x = rng.randint(60, W-60)
            y = rng.randint(60, H-60)
            rr = rng.randint(3, 9)
            ld.ellipse([x-rr,y-rr,x+rr,y+rr], fill=dot_col)
        for cx,cy,rr in [(220,160,110),(1120,520,140)]:
            ld.ellipse([cx-rr,cy-rr,cx+rr,cy+rr], fill=(accent[0],accent[1],accent[2],18))

    img = Image.alpha_composite(img.convert("RGBA"), layer).convert("RGB")
    draw = ImageDraw.Draw(img)

    # panel + shadow
    panel = (70,90,W-70,H-110)
    shadow = Image.new("RGBA",(W,H),(0,0,0,0))
    sd = ImageDraw.Draw(shadow)
    sd.rounded_rectangle([panel[0]+6,panel[1]+10,panel[2]+6,panel[3]+10], radius=28, fill=(0,0,0,60))
    shadow = shadow.filter(ImageFilter.GaussianBlur(10))
    img = Image.alpha_composite(img.convert("RGBA"), shadow).convert("RGB")
    draw = ImageDraw.Draw(img)

    draw.rounded_rectangle(panel, radius=28, fill=panel_fill, outline=panel_outline, width=2)

    # accent bar
    ax = panel[0]+28
    ay = panel[1]+28
    draw.rounded_rectangle([ax, ay, ax+10, panel[3]-28], radius=6, fill=accent)

    # typography
    f_title = _load_font(56 if theme!="scientific" else 54)
    f_sub = _load_font(32 if theme!="scientific" else 30)
    f_small = _load_font(24)

    x_text = ax+28
    y_text = panel[1]+44
    max_w = panel[2]-x_text-28

    def wrap(text: str, font: ImageFont.ImageFont, max_width: int) -> List[str]:
        words = (text or "").split()
        if not words:
            return []
        lines=[]; cur=[]
        for w in words:
            test = " ".join(cur+[w])
            if draw.textlength(test, font=font) <= max_width:
                cur.append(w)
            else:
                if cur:
                    lines.append(" ".join(cur))
                cur=[w]
        if cur:
            lines.append(" ".join(cur))
        return lines

    for ln in wrap(rubric_title, f_title, max_w)[:3]:
        draw.text((x_text, y_text), ln, fill=title_color, font=f_title)
        y_text += 68

    y_text += 12
    for ln in wrap(subtitle, f_sub, max_w)[:3]:
        draw.text((x_text, y_text), ln, fill=sub_color, font=f_sub)
        y_text += 44

    footer = (branding or {}).get("card_footer","")
    if footer:
        draw.text((panel[0]+28, panel[3]-48), footer, fill=footer_color, font=f_small)

    out = STATE_DIR/f"card_{sha1(theme+rubric_title+subtitle)[:10]}.png"
    img.save(out)
    return out

# ---------------------------
# Telegram helpers + stats + selection
# ---------------------------

def tg_request(method: str, data: Dict[str,Any], files: Optional[Dict[str,Any]]=None) -> Dict[str,Any]:
    if not TELEGRAM_BOT_TOKEN:
        raise RuntimeError("TELEGRAM_BOT_TOKEN is missing.")
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/{method}"
    r = requests.post(url, data=data, files=files, timeout=30)
    r.raise_for_status()
    return r.json()

def send_photo(chat_id: str, photo_path: Path, caption: str) -> None:
    with photo_path.open("rb") as f:
        tg_request("sendPhoto", data={"chat_id": chat_id, "caption": caption, "parse_mode":"Markdown"}, files={"photo": f})

def send_message(chat_id: str, text: str) -> None:
    tg_request("sendMessage", data={"chat_id": chat_id, "text": text, "parse_mode":"Markdown"})

def load_weekly_stats() -> Dict[str,Any]:
    return load_state("stats_weekly.json", {})

def save_weekly_stats(stats: Dict[str,Any]) -> None:
    save_state("stats_weekly.json", stats)

def bump_weekly(stats: Dict[str,Any], week_key: str, field: str, amount: int = 1, reason: Optional[str]=None) -> None:
    wk = stats.get(week_key) or {"passed": 0, "rejected": 0, "reasons": {}}
    wk[field] = int(wk.get(field,0)) + amount
    if reason:
        rs = wk.get("reasons") or {}
        rs[reason] = int(rs.get(reason,0)) + amount
        wk["reasons"] = rs
    stats[week_key] = wk

def format_dashboard(stats: Dict[str,Any], week_key: str, title: str) -> str:
    wk = stats.get(week_key) or {"passed": 0, "rejected": 0, "reasons": {}}
    passed = int(wk.get("passed",0))
    rejected = int(wk.get("rejected",0))
    reasons = wk.get("reasons") or {}
    top = sorted(reasons.items(), key=lambda x: x[1], reverse=True)[:6]
    lines = [f"**{title} ({week_key})**",
             "",
             f"‚úÖ –ü—Ä–æ—à–ª–æ: {passed}",
             f"üóÇÔ∏è –í —á–µ—Ä–Ω–æ–≤–∏–∫–∏/–æ—Ç—Å–µ–≤: {rejected}",
             ""]
    if top:
        lines.append("–ü—Ä–∏—á–∏–Ω—ã –æ—Ç—Å–µ–≤–∞ (—Ç–æ–ø):")
        for k,v in top:
            lines.append(f"‚Ä¢ {k}: {v}")
    else:
        lines.append("–ü—Ä–∏—á–∏–Ω—ã –æ—Ç—Å–µ–≤–∞: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö.")
    lines.append("")
    lines.append("_–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: —ç—Ç–æ —Ç–µ—Ö. —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤/—Ñ–∏–ª—å—Ç—Ä–æ–≤._")
    return "\n".join(lines)

def handle_draft(pub_cfg: Dict[str,Any], entry: Dict[str,Any], stats: Dict[str,Any], week_key: str) -> None:
    mode = (pub_cfg.get("drafts_mode") or "skip").strip()
    drafts_chat_id = ""
    if mode == "post_to_drafts_chat":
        env_name = pub_cfg.get("drafts_chat_id_env") or "TELEGRAM_DRAFTS_CHAT_ID"
        drafts_chat_id = os.getenv(env_name,"").strip() or TELEGRAM_DRAFTS_CHAT_ID

    drafts = load_state("drafts.json", [])
    drafts.append(entry)
    save_state("drafts.json", drafts[-2000:])

    bump_weekly(stats, week_key, "rejected", 1, reason=str(entry.get("reason","unknown")))

    if mode == "post_to_drafts_chat" and drafts_chat_id:
        msg = ("**–ß–µ—Ä–Ω–æ–≤–∏–∫/–ø—Ä–æ–ø—É—Å–∫**\n\n"
               f"–ü—Ä–∏—á–∏–Ω–∞: {entry.get('reason')}\n"
               f"–†—É–±—Ä–∏–∫–∞: {entry.get('rubric_title','')}\n"
               f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {entry.get('title')}\n"
               f"–°—Å—ã–ª–∫–∞: {entry.get('link')}\n")
        send_message(drafts_chat_id, msg)

def pick_item(items: List[Dict[str,str]], used_canon: set[str], used_titles: set[str], quality_cfg: Dict[str,Any]) -> Tuple[Optional[Dict[str,str]], Optional[Dict[str,Any]]]:
    ranked=[]
    for it in items:
        t = norm_space(it.get("title",""))
        l = it.get("link","")
        if not l:
            continue
        s,_ = score_item(t or "(no title)", l, quality_cfg)
        if s>=0:
            ranked.append((s,it))
    ranked.sort(key=lambda x:x[0], reverse=True)

    for _,it in ranked[:22]:
        it = enrich_article(dict(it))
        canon = it.get("canonical") or it.get("link","")
        if not canon or canon in used_canon:
            continue

        raw_title = it.get("article_title") or it.get("title") or ""
        tkey = norm_title_key(raw_title)
        if tkey and tkey in used_titles:
            continue

        dom = safe_domain(canon)
        summ = it.get("article_summary") or it.get("summary") or ""
        ok, reason = is_scientific_or_methodical(dom, raw_title, summ, quality_cfg)
        if not ok:
            return None, {
                "ts": datetime.utcnow().replace(tzinfo=timezone.utc).isoformat().replace("+00:00","Z"),
                "reason": f"fact_check_failed:{reason}",
                "title": raw_title,
                "link": canon,
                "domain": dom,
            }

        it["picked_title"]=raw_title
        it["picked_summary"]=summ
        it["fact_check"]=reason
        it["source_type"]=source_type_label_from_factcheck(reason)
        return it, None
    return None, None

def run() -> None:
    rub_cfg = load_yaml(CFG_DIR/"rubrics.yml")
    channel_cfg = rub_cfg.get("channel",{})
    branding = rub_cfg.get("branding",{})
    tzname = channel_cfg.get("timezone","Asia/Nicosia")
    now = get_local_now(tzname)
    week_key = iso_week_key(now)

    sources, quality_cfg = load_sources()
    used_canon = set(load_state("used_canonical.json",[]))
    used_titles = set(load_state("used_titles.json",[]))

    pub_cfg = rub_cfg.get("publishing",{})
    max_posts = int(pub_cfg.get("max_posts_per_run",3))
    max_per_aud = int(pub_cfg.get("max_posts_per_audience_per_run",2))

    stats = load_weekly_stats()

    audiences_cfg = rub_cfg.get("audiences",{})
    if AUDIENCE=="both":
        aud_list=["parents","pros"]
    elif AUDIENCE in ("parents","pros"):
        aud_list=[AUDIENCE]
    else:
        aud_list=["parents"]

    posted=0
    for aud in aud_list:
        if posted>=max_posts:
            break
        aud_cfg = audiences_cfg.get(aud,{})
        title_suffix = aud_cfg.get("title_suffix","")
        rubrics = aud_cfg.get("rubrics",[]) or []
        aud_posted=0

        for rubric in rubrics:
            if posted>=max_posts or aud_posted>=max_per_aud:
                break
            if not is_due(rubric, now):
                continue

            if rubric.get("format") == "quality_dashboard":
                dash_title = pub_cfg.get("dashboard_title","Quality dashboard –Ω–µ–¥–µ–ª–∏")
                dashboard_text = format_dashboard(stats, week_key, dash_title)
                dash_chat = (pub_cfg.get("dashboard_chat") or "main").strip().lower()
                chat_id = TELEGRAM_CHAT_ID
                if dash_chat == "drafts" and TELEGRAM_DRAFTS_CHAT_ID:
                    chat_id = TELEGRAM_DRAFTS_CHAT_ID
                send_message(chat_id, dashboard_text)
                time.sleep(0.7)
                continue

            all_items=[]
            for sid in rubric.get("sources",[]):
                src = sources.get(sid)
                if not src:
                    continue
                try:
                    all_items.extend(fetch_source(src))
                except Exception as e:
                    print(f"[WARN] source {sid} failed: {e}")

            picked, draft = pick_item(all_items, used_canon, used_titles, quality_cfg)
            if draft:
                draft.update({"audience": aud, "rubric": rubric.get("id",""), "rubric_title": rubric.get("title","")})
                handle_draft(pub_cfg, draft, stats, week_key)
                continue
            if not picked:
                continue

            title = rubric.get("title","–†—É–±—Ä–∏–∫–∞")
            text = make_text(title, rubric.get("format",""), aud, channel_cfg, picked, title_suffix)

            subtitle = "–ö–æ—Ä–æ—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É"
            summ = (picked.get("picked_summary") or "").strip()
            if summ:
                subtitle = summ[:110].rstrip(" .,:;‚Äî-") + "‚Ä¶"

            card = render_image_card(title, subtitle, branding)
            send_photo(TELEGRAM_CHAT_ID, card, text[:950])

            bump_weekly(stats, week_key, "passed", 1)

            canon = picked.get("canonical") or picked.get("link","")
            if canon: used_canon.add(canon)
            tkey = norm_title_key(picked.get("picked_title") or picked.get("title") or "")
            if tkey: used_titles.add(tkey)

            posted += 1
            aud_posted += 1
            time.sleep(1.2)

    save_state("used_canonical.json", sorted(list(used_canon))[-6000:])
    save_state("used_titles.json", sorted(list(used_titles))[-6000:])
    save_weekly_stats(stats)

    print(f"Done. Posted: {posted}. Audience: {AUDIENCE}. Rewrite: {REWRITE_PROVIDER}. Week: {week_key}")

if __name__=="__main__":
    run()
