from __future__ import annotations

import os, re, json, time, random, hashlib, math
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlparse, urljoin

import requests, yaml, feedparser
from bs4 import BeautifulSoup
from dateutil import tz
from PIL import Image, ImageDraw, ImageFont, ImageFilter

ROOT = Path(__file__).resolve().parents[1]
CFG_DIR = ROOT / "config"
STATE_DIR = ROOT / ".state"
ASSETS_DIR = ROOT / "assets"
FONTS_DIR = ASSETS_DIR / "fonts"
STATE_DIR.mkdir(exist_ok=True)

USER_AGENT = "logoped-channel-bot/1.6.1 (+https://github.com/)"
HEADERS = {"User-Agent": USER_AGENT}

TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN","").strip()
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID","").strip()
TELEGRAM_DRAFTS_CHAT_ID = os.getenv("TELEGRAM_DRAFTS_CHAT_ID","").strip()

REWRITE_PROVIDER = os.getenv("REWRITE_PROVIDER","auto").strip().lower()  # none|auto|groq|gemini
GROQ_API_KEY = os.getenv("GROQ_API_KEY","").strip()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY","").strip()

AUDIENCE = os.getenv("AUDIENCE","parents").strip().lower()  # parents|pros|both

# v1.6.1: style/length knobs (Telegram photo caption is limited; we keep a conservative target)
PARENTS_MAX_BODY_CHARS = int(os.getenv("PARENTS_MAX_BODY_CHARS","860"))
PROS_MAX_BODY_CHARS = int(os.getenv("PROS_MAX_BODY_CHARS","980"))

# v1.6.1: quality gate knobs
MIN_MEANING_BULLETS = int(os.getenv("MIN_MEANING_BULLETS","2"))
MIN_PRACTICE_STEPS = int(os.getenv("MIN_PRACTICE_STEPS","3"))

@dataclass
class Source:
    id: str
    name: str
    type: str
    url: Optional[str] = None
    urls: Optional[List[str]] = None
    parser: Optional[str] = None
    notes: str = ""

def load_yaml(path: Path) -> Dict[str,Any]:
    return yaml.safe_load(path.read_text(encoding="utf-8"))

def norm_space(s: str) -> str:
    return re.sub(r"\s+"," ",(s or "").strip())

def clamp_text(s: str, max_len: int) -> str:
    s = norm_space(s)
    if len(s) <= max_len:
        return s
    return (s[:max_len].rstrip(" .,:;â€”-") + "â€¦").strip()

def markdown_to_plain(text: str) -> str:
    """
    Telegram Markdown Ñ‡Ð°ÑÑ‚Ð¾ Ð¿Ð°Ð´Ð°ÐµÑ‚ Ð½Ð° URL Ñ '_' Ð¸ Ð¿Ñ€Ð¾Ñ‡Ð¸Ð¼Ð¸ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð°Ð¼Ð¸.
    ÐŸÐ¾ÑÑ‚Ð¾Ð¼Ñƒ Ð¿Ñ€Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐµ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ð¼Ñ‹ Ð¾Ñ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ plain-text Ð±ÐµÐ· parse_mode.
    """
    t = text or ""
    # ÑƒÐ±Ð¸Ñ€Ð°ÐµÐ¼ Ñ‚Ð¸Ð¿Ð¾Ð²Ñ‹Ðµ Ð¼Ð°Ñ€ÐºÐµÑ€Ñ‹ Markdown
    t = t.replace("**", "")
    t = t.replace("__", "")
    t = t.replace("`", "")
    # Ð¾Ð´Ð¸Ð½Ð¾Ñ‡Ð½Ñ‹Ðµ Ð¼Ð°Ñ€ÐºÐµÑ€Ñ‹
    t = t.replace("*", "")
    t = t.replace("_", "")
    # Ð¸Ð½Ð¾Ð³Ð´Ð° Ð²ÑÑ‚Ñ€ÐµÑ‡Ð°ÑŽÑ‚ÑÑ "###" Ð¸ Ñ‚.Ð¿.
    t = re.sub(r"^\s{0,3}#{1,6}\s*", "", t, flags=re.MULTILINE)
    return t.strip()

def norm_title_key(s: str) -> str:
    s = (s or "").lower()
    s = re.sub(r"[^\w\s]+"," ",s,flags=re.UNICODE)
    s = re.sub(r"\s+"," ",s).strip()
    s = re.sub(r"\b(Ð»Ð¾Ð³Ð¾Ð¿ÐµÐ´|Ð»Ð¾Ð³Ð¾Ð¿ÐµÐ´Ð¸Ñ|Ð»Ð¾Ð³Ð¾Ð¿ÐµÐ´Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹|ÑƒÐ¿Ñ€Ð°Ð¶Ð½ÐµÐ½Ð¸Ðµ|ÑƒÐ¿Ñ€Ð°Ð¶Ð½ÐµÐ½Ð¸Ñ)\b","",s).strip()
    s = re.sub(r"\s+"," ",s).strip()
    return s[:180]

def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8")).hexdigest()

def get_local_now(tzname: str) -> datetime:
    return datetime.now(tz=tz.gettz(tzname))

def iso_week_key(dt: datetime) -> str:
    y, w, _ = dt.isocalendar()
    return f"{y}-W{w:02d}"

def is_due(rubric: Dict[str,Any], now: datetime) -> bool:
    cadence = (rubric.get("cadence") or "DAILY").upper()
    if cadence == "DAILY":
        return True
    if cadence == "WEEKLY":
        byweekday = set(rubric.get("byweekday") or [])
        map_wd = ["MO","TU","WE","TH","FR","SA","SU"]
        return map_wd[now.weekday()] in byweekday
    return False

def load_sources() -> Tuple[Dict[str,Source], Dict[str,Any]]:
    cfg = load_yaml(CFG_DIR/"sources.yml")
    quality = cfg.get("quality",{})
    out: Dict[str,Source] = {}
    for s in cfg.get("sources",[]):
        out[s["id"]] = Source(**s)
    return out, quality

def load_state(name: str, default: Any) -> Any:
    p = STATE_DIR/name
    if not p.exists():
        return default
    try:
        return json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        return default

def save_state(name: str, data: Any) -> None:
    (STATE_DIR/name).write_text(json.dumps(data,ensure_ascii=False,indent=2),encoding="utf-8")

def safe_domain(url: str) -> str:
    try:
        return (urlparse(url).netloc or "").lower()
    except Exception:
        return ""

def domain_allowed(url: str, allow_domains: List[str]) -> bool:
    d = safe_domain(url)
    return bool(d) and any(d==ad or d.endswith("."+ad) for ad in allow_domains)

def score_item(title: str, link: str, quality_cfg: Dict[str,Any]) -> Tuple[int,str]:
    t = (title or "").strip()
    u = (link or "").strip()
    if len(t) < 12 or len(t) > 240:
        return (-100,"bad_title_len")
    allow_domains = quality_cfg.get("allow_domains") or []
    if allow_domains and not domain_allowed(u, allow_domains):
        return (-100,"domain_not_allowed")
    tl, ul = t.lower(), u.lower()
    for k in [x.lower() for x in (quality_cfg.get("deny_keywords") or [])]:
        if k and (k in tl or k in ul):
            return (-100,f"deny_keyword:{k}")
    score = 10
    for k in [x.lower() for x in (quality_cfg.get("boost_keywords") or [])]:
        if k and k in tl:
            score += 2
    return (score,"ok")

def http_get(url: str, timeout: int = 25) -> requests.Response:
    """
    Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ð¹ GET: Ð´Ð»Ñ logorina.ru Ð¿Ñ€Ð¸ CERT_VERIFY Ð´ÐµÐ»Ð°ÐµÐ¼ ÐµÐ´Ð¸Ð½Ð¸Ñ‡Ð½Ñ‹Ð¹ Ñ€ÐµÑ‚Ñ€Ð°Ð¹ verify=False.
    """
    try:
        return requests.get(url, headers=HEADERS, timeout=timeout)
    except requests.exceptions.SSLError:
        d = safe_domain(url)
        if d.endswith("logorina.ru"):
            print("[WARN] SSL verify failed for logorina.ru; retry with verify=False")
            return requests.get(url, headers=HEADERS, timeout=timeout, verify=False)
        raise

def get_canonical_and_soup(url: str) -> Tuple[str, Optional[BeautifulSoup]]:
    try:
        r = http_get(url, timeout=25)
        r.raise_for_status()
        soup = BeautifulSoup(r.text,"lxml")
        canon = soup.find("link", rel=lambda x: x and "canonical" in x.lower())
        if canon and canon.get("href"):
            href = canon["href"].strip()
            if href.startswith("/"):
                href = urljoin(url, href)
            return href, soup
        return url, soup
    except Exception:
        return url, None

def extract_article_title(soup: BeautifulSoup) -> str:
    og = soup.find("meta", property="og:title")
    if og and og.get("content"):
        return norm_space(og["content"])
    h1 = soup.find("h1")
    if h1:
        return norm_space(h1.get_text(" ",strip=True))
    if soup.title and soup.title.string:
        return norm_space(soup.title.string)
    return ""

def extract_article_summary(soup: BeautifulSoup) -> str:
    md = soup.find("meta", attrs={"name":"description"})
    if md and md.get("content"):
        return norm_space(md["content"])
    ogd = soup.find("meta", property="og:description")
    if ogd and ogd.get("content"):
        return norm_space(ogd["content"])
    paras=[]
    for p in soup.select("p"):
        txt = norm_space(p.get_text(" ",strip=True))
        if len(txt) < 60:
            continue
        if any(bad in txt.lower() for bad in ["cookie","Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸Ðº","Ð¿Ð¾Ð´Ð¿Ð¸Ñ","Ñ€ÐµÐºÐ»Ð°Ð¼Ð°"]):
            continue
        paras.append(txt)
        if len(paras)>=2:
            break
    return norm_space(" ".join(paras))[:420]

def is_scientific_or_methodical(domain: str, title: str, summary: str, quality_cfg: Dict[str,Any]) -> Tuple[bool,str]:
    scientific_domains = [d.lower() for d in (quality_cfg.get("scientific_domains") or [])]
    if any(domain==d or domain.endswith("."+d) for d in scientific_domains):
        return True,"scientific_domain"
    blob = f"{title}\n{summary}".lower()
    kws = [k.lower() for k in (quality_cfg.get("methodical_keywords") or [])]
    hits = sum(1 for k in kws if k and k in blob)
    if hits >= 2:
        return True,f"methodical_kw_hits:{hits}"
    return False,f"not_methodical_hits:{hits}"

def source_type_label_from_factcheck(factcheck_reason: str) -> str:
    r = (factcheck_reason or "").lower()
    if "scientific_domain" in r:
        return "Ð½Ð°ÑƒÑ‡Ð½Ñ‹Ð¹/Ð°ÐºÐ°Ð´ÐµÐ¼Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº"
    return "Ð¼ÐµÑ‚Ð¾Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹/Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»"

# ---------------------------
# Site-specific parsers (v1.4)
# ---------------------------

def _abs(url: str, href: str) -> str:
    href = (href or "").strip()
    if not href:
        return ""
    if href.startswith("//"):
        return "https:" + href
    if href.startswith("/"):
        return urljoin(url, href)
    if href.startswith("http://") or href.startswith("https://"):
        return href
    return urljoin(url, href)

def _collect_links(base_url: str, soup: BeautifulSoup, selector: str, href_re: Optional[str]=None) -> List[Dict[str,str]]:
    pat = re.compile(href_re) if href_re else None
    out=[]
    for a in soup.select(selector):
        href = _abs(base_url, a.get("href",""))
        if not href:
            continue
        if pat and not pat.search(href):
            continue
        title = norm_space(a.get_text(" ", strip=True))
        if not title or len(title) < 8:
            continue
        out.append({"title": title, "link": href, "summary": ""})
    seen=set(); uniq=[]
    for it in out:
        if it["link"] in seen:
            continue
        seen.add(it["link"])
        uniq.append(it)
    return uniq

def parse_logopediya_publ(url: str, html: str) -> List[Dict[str,str]]:
    soup = BeautifulSoup(html, "lxml")
    items = _collect_links(url, soup, "div#dle-content a, div#dle-content h2 a, div#dle-content h3 a", r"/publ/[^\"']+")
    items = [it for it in items if not re.search(r"/page/\d+/?$", it["link"])]
    return items[:80]

def parse_logorina_news(url: str, html: str) -> List[Dict[str,str]]:
    soup = BeautifulSoup(html, "lxml")
    items = _collect_links(url, soup, "article a, div.news a, a", r"/news/[\w\-]+/?$")
    return items[:80]

def parse_logomag_lib(url: str, html: str) -> List[Dict[str,str]]:
    soup = BeautifulSoup(html, "lxml")
    items = _collect_links(url, soup, "main a, div.content a, a", r"/lib/[^\"']+")
    return items[:80]

def parse_logoportal_articles(url: str, html: str) -> List[Dict[str,str]]:
    soup = BeautifulSoup(html, "lxml")
    items = _collect_links(url, soup, "main a, div#content a, article a, a", r"(statya-|/statya-)")
    return items[:80]

def parse_logopedy_articles(url: str, html: str) -> List[Dict[str,str]]:
    soup = BeautifulSoup(html, "lxml")
    items = _collect_links(url, soup, "div.content a, main a, a", r"logoped-article|logoped-literature|portal/[^#]+")
    items.sort(key=lambda x: len(x["title"]), reverse=True)
    return items[:80]

SITE_PARSERS = {
    "logopediya_publ": parse_logopediya_publ,
    "logorina_news": parse_logorina_news,
    "logomag_lib": parse_logomag_lib,
    "logoportal_articles": parse_logoportal_articles,
    "logopedy_articles": parse_logopedy_articles,
}

def fetch_rss(url: str) -> List[Dict[str,str]]:
    d = feedparser.parse(url)
    out=[]
    for e in d.entries[:50]:
        out.append({
            "title": norm_space(getattr(e,"title","")),
            "link": getattr(e,"link",""),
            "summary": norm_space(re.sub("<.*?>","",getattr(e,"summary",""))),
        })
    return out

def fetch_static(urls: List[str]) -> List[Dict[str,str]]:
    return [{"title":"","link":u,"summary":""} for u in urls]

def fetch_html_site(url: str, parser_name: str) -> List[Dict[str,str]]:
    r = http_get(url, timeout=30)
    r.raise_for_status()
    parser = SITE_PARSERS.get(parser_name)
    if not parser:
        raise ValueError(f"Unknown site parser: {parser_name}")
    items = parser(url, r.text)
    uniq={}
    for it in items:
        uniq[it["link"]] = it
    return list(uniq.values())

def fetch_source(src: Source) -> List[Dict[str,str]]:
    if src.type=="rss":
        return fetch_rss(src.url or "")
    if src.type=="html_site":
        return fetch_html_site(src.url or "", src.parser or "")
    if src.type=="static":
        return fetch_static(src.urls or [])
    raise ValueError(f"Unsupported source type: {src.type}")

def enrich_article(item: Dict[str,str]) -> Dict[str,str]:
    link = item.get("link","")
    canon, soup = get_canonical_and_soup(link)
    item["canonical"]=canon
    if soup:
        at = extract_article_title(soup)
        if at: item["article_title"]=at
        sm = extract_article_summary(soup)
        if sm: item["article_summary"]=sm
    return item

# ---------------------------
# LLM rewriting (v1.6.1 prompts v2)
# ---------------------------

def _is_quota_error(status: int, text: str) -> bool:
    t=(text or "").lower()
    return status in (402,429) or any(k in t for k in ["quota","rate limit","exceeded","insufficient_quota","resource_exhausted"])

def rewrite_with_groq(prompt: str) -> str:
    if not GROQ_API_KEY: raise RuntimeError("GROQ_API_KEY missing")
    r = requests.post(
        "https://api.groq.com/openai/v1/chat/completions",
        headers={"Authorization": f"Bearer {GROQ_API_KEY}", "Content-Type":"application/json"},
        json={
            "model":"llama-3.1-8b-instant",
            "messages":[{"role":"user","content":prompt}],
            "temperature":0.35
        },
        timeout=45
    )
    if r.status_code!=200 and _is_quota_error(r.status_code,r.text):
        raise RuntimeError(f"groq_quota:{r.status_code}")
    r.raise_for_status()
    return r.json()["choices"][0]["message"]["content"].strip()

def rewrite_with_gemini(prompt: str) -> str:
    if not GEMINI_API_KEY: raise RuntimeError("GEMINI_API_KEY missing")
    r = requests.post(
        "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent",
        params={"key": GEMINI_API_KEY},
        json={"contents":[{"parts":[{"text":prompt}]}]},
        timeout=45
    )
    if r.status_code!=200 and _is_quota_error(r.status_code,r.text):
        raise RuntimeError(f"gemini_quota:{r.status_code}")
    r.raise_for_status()
    return r.json()["candidates"][0]["content"]["parts"][0]["text"].strip()

def _aud_limits(audience: str) -> int:
    a = (audience or "parents").strip().lower()
    return PARENTS_MAX_BODY_CHARS if a == "parents" else PROS_MAX_BODY_CHARS

def _build_rewrite_prompt_v2(body: str, audience: str, max_chars: int) -> str:
    a = (audience or "parents").strip().lower()

    common_rules = (
        "Ð¢Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ:\n"
        "1) Ð ÑƒÑÑÐºÐ¸Ð¹ ÑÐ·Ñ‹Ðº. ÐÐµÐ¹Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ð¾-Ð½Ð°ÑƒÑ‡Ð½Ñ‹Ð¹, Ð±ÐµÑ€ÐµÐ¶Ð½Ñ‹Ð¹ Ñ‚Ð¾Ð½.\n"
        "2) ÐÐ• ÑÑ‚Ð°Ð²ÑŒ Ð´Ð¸Ð°Ð³Ð½Ð¾Ð·Ñ‹, ÐÐ• Ð¾Ð±ÐµÑ‰Ð°Ð¹ Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ, ÐÐ• Ð½Ð°Ð·Ð½Ð°Ñ‡Ð°Ð¹ Ð¿Ñ€ÐµÐ¿Ð°Ñ€Ð°Ñ‚Ñ‹.\n"
        "3) ÐÐµ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐ¹ Ð½Ð¾Ð²Ñ‹Ñ… Ñ„Ð°ÐºÑ‚Ð¾Ð². Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ð¿ÐµÑ€ÐµÑ„Ñ€Ð°Ð·Ð¸Ñ€ÑƒÐ¹.\n"
        "4) Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ Ð¸ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº ÑÐµÐºÑ†Ð¸Ð¹ Ð¸ ÑÐ¿Ð¸ÑÐºÐ¾Ð².\n"
        "5) ÐÐµ Ð¼ÐµÐ½ÑÐ¹ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ ÑÐµÐºÑ†Ð¸Ð¹ Ð¸ Ð½Ðµ ÑƒÐ´Ð°Ð»ÑÐ¹ Ð¸Ñ….\n"
        "6) ÐÐµ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐ¹ Ð½Ð¾Ð²Ñ‹Ñ… Ñ€Ð°Ð·Ð´ÐµÐ»Ð¾Ð².\n"
        f"7) Ð”Ð»Ð¸Ð½Ð° Ñ‚ÐµÐ»Ð° Ñ‚ÐµÐºÑÑ‚Ð° (Ð±ÐµÐ· Ð±Ð»Ð¾ÐºÐ° Â«Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÂ» Ð¸ Ð±ÐµÐ· Ð´Ð¸ÑÐºÐ»ÐµÐ¹Ð¼ÐµÑ€Ð°/Ñ…ÐµÑˆÑ‚ÐµÐ³Ð¾Ð²): Ð´Ð¾ {max_chars} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð².\n"
        "Ð¡ÐµÐºÑ†Ð¸Ð¸ Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ Ñ€Ð¾Ð²Ð½Ð¾ Ñ‚Ð°ÐºÐ¸Ð¼Ð¸:\n"
        "Â«Ð¡ÑƒÑ‚ÑŒÂ», Â«Ð§Ñ‚Ð¾ ÑÑ‚Ð¾ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ Ð´Ð»Ñ Ð²Ð°ÑÂ», Â«ÐŸÑ€Ð°ÐºÑ‚Ð¸ÐºÐ° Ð½Ð° ÑÐµÐ³Ð¾Ð´Ð½Ñ (5â€“7 Ð¼Ð¸Ð½ÑƒÑ‚)Â», Â«ÐÐ¾Ñ€Ð¼Ð° / ÐºÐ¾Ð³Ð´Ð° Ð½ÑƒÐ¶ÐµÐ½ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸ÑÑ‚Â».\n"
    )

    if a == "pros":
        style = (
            "ÐÑƒÐ´Ð¸Ñ‚Ð¾Ñ€Ð¸Ñ: ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸ÑÑ‚Ñ‹ (Ð»Ð¾Ð³Ð¾Ð¿ÐµÐ´Ñ‹/Ð´ÐµÑ„ÐµÐºÑ‚Ð¾Ð»Ð¾Ð³Ð¸).\n"
            "Ð¡Ñ‚Ð¸Ð»ÑŒ: Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾, Ñ‚Ð¾Ñ‡Ð½ÐµÐµ Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ñ‹, Ð½Ð¾ Ð±ÐµÐ· ÐºÐ°Ð½Ñ†ÐµÐ»ÑÑ€Ð¸Ñ‚Ð°. "
            "ÐœÐ¾Ð¶Ð½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ ÑƒÐ¼ÐµÑ€ÐµÐ½Ð½ÑƒÑŽ Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑŽ (Ñ„Ð¾Ð½ÐµÐ¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ ÑÐ»ÑƒÑ…, Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»ÑÑ†Ð¸Ð¾Ð½Ð½Ð°Ñ Ð¼Ð¾Ñ‚Ð¾Ñ€Ð¸ÐºÐ°, Ð»ÐµÐºÑÐ¸ÐºÐ¾-Ð³Ñ€Ð°Ð¼Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ ÑÑ‚Ñ€Ð¾Ð¹), "
            "Ð½Ð¾ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²ÐºÐ¸ Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð¾ÑÑ‚Ð°Ð²Ð°Ñ‚ÑŒÑÑ ÑÑÐ½Ñ‹Ð¼Ð¸.\n"
        )
    else:
        style = (
            "ÐÑƒÐ´Ð¸Ñ‚Ð¾Ñ€Ð¸Ñ: Ñ€Ð¾Ð´Ð¸Ñ‚ÐµÐ»Ð¸.\n"
            "Ð¡Ñ‚Ð¸Ð»ÑŒ: Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð½Ñ‹Ð¹, Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ðµ ÑÐ»Ð¾Ð²Ð°, Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÑŽÑ‰Ð¸Ð¹ Ñ‚Ð¾Ð½. "
            "Ð£Ð±Ð¸Ñ€Ð°Ð¹ ÐºÐ°Ð½Ñ†ÐµÐ»ÑÑ€Ð¸Ñ‚ Ð¸ â€˜ÑƒÐ¼Ð½Ñ‹Ðµâ€™ Ð¾Ð±Ð¾Ñ€Ð¾Ñ‚Ñ‹. "
            "Ð•ÑÐ»Ð¸ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð°ÐµÑ‚ÑÑ Ñ‚ÐµÑ€Ð¼Ð¸Ð½ â€” ÐºÑ€Ð°Ñ‚ÐºÐ¾ Ð¾Ð±ÑŠÑÑÐ½Ð¸ Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ð¼Ð¸ ÑÐ»Ð¾Ð²Ð°Ð¼Ð¸ Ð² Ñ‚Ð¾Ð¹ Ð¶Ðµ Ñ„Ñ€Ð°Ð·Ðµ.\n"
        )

    return (
        style
        + common_rules
        + "\nÐ¢Ð•ÐšÐ¡Ð¢ Ð”Ð›Ð¯ ÐŸÐ•Ð Ð•Ð¤ÐžÐ ÐœÐ£Ð›Ð˜Ð ÐžÐ’ÐšÐ˜:\n"
        + body.strip()
    )

def _enforce_body_limit_v2(text: str, max_chars: int) -> str:
    t = text.strip()
    if len(t) <= max_chars:
        return t
    # Ð°ÐºÐºÑƒÑ€Ð°Ñ‚Ð½Ð¾ ÑƒÑ€ÐµÐ·Ð°ÐµÐ¼ ÐºÐ¾Ð½ÐµÑ†, ÑÑ‚Ð°Ñ€Ð°ÑÑÑŒ Ð½Ðµ Ð»Ð¾Ð¼Ð°Ñ‚ÑŒ Ð±Ð»Ð¾ÐºÐ¸: Ð¾Ð±Ñ€ÐµÐ·Ð°ÐµÐ¼ Ð¿Ð¾ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ¹ Ð³Ñ€Ð°Ð½Ð¸Ñ†Ðµ ÑÑ‚Ñ€Ð¾ÐºÐ¸
    cut = t[:max_chars]
    if "\n" in cut:
        cut = cut[:cut.rfind("\n")].rstrip()
    return (cut.rstrip(" .,:;â€”-") + "â€¦").strip()

def rewrite_if_enabled(text: str, audience: str) -> str:
    if REWRITE_PROVIDER=="none":
        return text

    # v1.6+: protect Source + disclaimer + hashtags from rewriting
    marker = "\n**Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº**\n"
    idx = text.find(marker)

    if idx != -1:
        body = text[:idx].strip()
        tail = text[idx:].strip()
    else:
        # legacy fallback (older template)
        parts = text.split("Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº:",1)
        body = parts[0].strip()
        tail = ("Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº:"+parts[1]).strip() if len(parts)==2 else ""

    max_chars = _aud_limits(audience)
    prompt = _build_rewrite_prompt_v2(body, audience, max_chars)

    try:
        if REWRITE_PROVIDER in ("groq","auto"):
            try:
                out = rewrite_with_groq(prompt)
                out = _enforce_body_limit_v2(out, max_chars)
                return out + ("\n\n"+tail if tail else "")
            except Exception as e:
                if REWRITE_PROVIDER=="groq":
                    raise
                if "groq_quota" in str(e):
                    print("[WARN] groq quota; fallback to gemini")
                else:
                    print(f"[WARN] groq rewrite failed: {e}")
        if REWRITE_PROVIDER in ("gemini","auto"):
            out = rewrite_with_gemini(prompt)
            out = _enforce_body_limit_v2(out, max_chars)
            return out + ("\n\n"+tail if tail else "")
    except Exception as e:
        print(f"[WARN] rewrite failed ({REWRITE_PROVIDER}): {e}")
        return text

    return text

# ---------------------------
# Post template v2 + quality gate (v1.6.1)
# ---------------------------

def make_question_week() -> str:
    questions = [
        "Ð ÐµÐ±Ñ‘Ð½Ð¾Ðº Ð¿Ð¾Ð½Ð¸Ð¼Ð°ÐµÑ‚ Ð¾Ð±Ñ€Ð°Ñ‰Ñ‘Ð½Ð½ÑƒÑŽ Ñ€ÐµÑ‡ÑŒ, Ð½Ð¾ Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ñ‚ Ð¼Ð°Ð»Ð¾: ÐºÐ°ÐºÐ¸Ðµ ÑˆÐ°Ð³Ð¸ Ð²Ñ‹ ÑƒÐ¶Ðµ Ð¿Ñ€Ð¾Ð±Ð¾Ð²Ð°Ð»Ð¸ Ð´Ð¾Ð¼Ð°?",
        "Ð’ Ð±Ð¸Ð»Ð¸Ð½Ð³Ð²Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÑÐµÐ¼ÑŒÐµ: Ð½Ð° ÐºÐ°ÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ Ñ€ÐµÐ±Ñ‘Ð½ÐºÑƒ Ð»ÐµÐ³Ñ‡Ðµ Ñ€Ð°ÑÑÐºÐ°Ð·Ñ‹Ð²Ð°Ñ‚ÑŒ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ð¸ Ð¿Ð¾Ñ‡ÐµÐ¼Ñƒ?",
        "ÐšÐ°ÐºÐ¸Ðµ Ð·Ð²ÑƒÐºÐ¸/ÑÐ»Ð¾Ð³Ð¸ Ð´Ð°ÑŽÑ‚ÑÑ Ñ‚Ñ€ÑƒÐ´Ð½ÐµÐµ Ð²ÑÐµÐ³Ð¾ â€” Ð¸ Ð² ÐºÐ°ÐºÐ¸Ñ… ÑÐ»Ð¾Ð²Ð°Ñ… ÑÑ‚Ð¾ Ð·Ð°Ð¼ÐµÑ‚Ð½ÐµÐµ?",
        "Ð§Ñ‚Ð¾ Ð²Ñ‹Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð±Ð¾Ð»ÑŒÑˆÐµ ÑÐ¾Ð¿Ñ€Ð¾Ñ‚Ð¸Ð²Ð»ÐµÐ½Ð¸Ñ: Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»ÑÑ†Ð¸Ð¾Ð½Ð½Ð°Ñ Ð³Ð¸Ð¼Ð½Ð°ÑÑ‚Ð¸ÐºÐ°, Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÐµÐ½Ð¸Ðµ ÑÐ»Ð¾Ð³Ð¾Ð² Ð¸Ð»Ð¸ Ñ‡Ñ‚ÐµÐ½Ð¸Ðµ/Ð¿Ð¸ÑÑŒÐ¼Ð¾?",
        "ÐšÐ°Ðº Ð²Ñ‹Ð³Ð»ÑÐ´Ð¸Ñ‚ Ð²Ð°Ñˆ Â«Ð¸Ð´ÐµÐ°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Â» Ñ‡ÐµÑ€ÐµÐ· 4 Ð½ÐµÐ´ÐµÐ»Ð¸ Ð·Ð°Ð½ÑÑ‚Ð¸Ð¹ â€” Ð² Ð¾Ð´Ð½Ð¾Ð¼ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¸?",
    ]
    return random.choice(questions)

def _bullets(lines: List[str]) -> str:
    clean = [norm_space(x) for x in lines if norm_space(x)]
    return "\n".join([f"â€¢ {x}" for x in clean])

def _numbered(lines: List[str]) -> str:
    clean = [norm_space(x) for x in lines if norm_space(x)]
    return "\n".join([f"{i+1}) {x}" for i,x in enumerate(clean)])

def _has_required_headings(text: str) -> bool:
    # v1.6.1: ensure structure is intact after rewrite (or no-rewrite)
    required = [
        "**Ð¡ÑƒÑ‚ÑŒ**",
        "**Ð§Ñ‚Ð¾ ÑÑ‚Ð¾ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ Ð´Ð»Ñ Ð²Ð°Ñ**",
        "**ÐŸÑ€Ð°ÐºÑ‚Ð¸ÐºÐ° Ð½Ð° ÑÐµÐ³Ð¾Ð´Ð½Ñ (5â€“7 Ð¼Ð¸Ð½ÑƒÑ‚)**",
        "**ÐÐ¾Ñ€Ð¼Ð° / ÐºÐ¾Ð³Ð´Ð° Ð½ÑƒÐ¶ÐµÐ½ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸ÑÑ‚**",
        "**Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº**",
    ]
    return all(r in text for r in required)

def _quality_gate(
    rubric_format: str,
    audience: str,
    link: str,
    essence: str,
    meaning: List[str],
    practice: List[str],
    norm_lines: List[str],
) -> Tuple[bool, str]:
    rf = (rubric_format or "").strip().lower()
    aud = (audience or "parents").strip().lower()

    if not link or not link.startswith(("http://","https://")):
        return False, "quality_gate:no_source_link"

    # essence should not be empty/too short (except question_week which can be compact)
    ess_len = len(norm_space(essence))
    if rf != "question_week" and ess_len < 40:
        return False, f"quality_gate:weak_essence_len:{ess_len}"
    if rf == "question_week" and ess_len < 25:
        return False, f"quality_gate:weak_question_len:{ess_len}"

    # meaning bullets
    m = [x for x in meaning if norm_space(x)]
    if len(m) < MIN_MEANING_BULLETS:
        return False, f"quality_gate:meaning_bullets_lt_{MIN_MEANING_BULLETS}:{len(m)}"

    # practice steps
    p = [x for x in practice if norm_space(x)]
    if len(p) < MIN_PRACTICE_STEPS:
        return False, f"quality_gate:practice_steps_lt_{MIN_PRACTICE_STEPS}:{len(p)}"

    # norm lines: must include both normal and consult hint
    nl = "\n".join([norm_space(x) for x in norm_lines if norm_space(x)])
    if "âœ…" not in nl or "âš ï¸" not in nl:
        return False, "quality_gate:norm_block_missing_markers"

    # audience nuance: for pros, require at least one actionable pro-oriented line in practice for pro rubrics
    if aud == "pros" and rf in ("pro_friendly","case_digest"):
        blob = " ".join(p).lower()
        if not any(k in blob for k in ["Ñ†ÐµÐ»ÑŒ", "ÐºÑ€Ð¸Ñ‚ÐµÑ€", "Ñ‡ÐµÐº", "ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»", "Ð¾Ð½Ð»Ð°Ð¹Ð½", "Ð¿Ð»Ð°Ð½"]):
            return False, "quality_gate:pros_practice_too_generic"

    return True, "ok"

def compose_post_v2(
    rubric_title: str,
    rubric_format: str,
    audience: str,
    channel_cfg: Dict[str,Any],
    picked: Dict[str,str],
    title_suffix: str
) -> Tuple[str, Dict[str,Any]]:
    """
    v1.6 â€” post_template_v2 with mandatory blocks for all rubrics (except quality_dashboard).
    v1.6.1 â€” prompts v2 + quality gate + structure validation.
    """
    link = picked.get("canonical") or picked.get("link","")
    picked_title = picked.get("picked_title") or picked.get("title") or ""
    summary = picked.get("picked_summary") or picked.get("summary") or ""
    disclaimer = channel_cfg.get("disclaimer","")
    tags = " ".join(channel_cfg.get("hashtags",[]))

    aud = (audience or "parents").strip().lower()
    rf = (rubric_format or "").strip().lower()

    # clamp article title/summary to keep posts compact and consistent
    picked_title_c = clamp_text(picked_title, 140) if picked_title else ""
    summary_c = clamp_text(summary, 240) if summary else ""

    # --- Ð¡ÑƒÑ‚ÑŒ
    if rf == "question_week":
        q = make_question_week()
        essence = (
            "ÐÐµÐ±Ð¾Ð»ÑŒÑˆÐ¾Ð¹ â€œÐ²Ð¾Ð¿Ñ€Ð¾Ñ Ð½ÐµÐ´ÐµÐ»Ð¸â€ â€” Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¼ÑÐ³ÐºÐ¾ Ð¿Ð¾Ð½ÑÑ‚ÑŒ Ñ‚ÐµÐºÑƒÑ‰ÑƒÑŽ ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸ÑŽ Ð¸ Ð²Ñ‹Ð±Ñ€Ð°Ñ‚ÑŒ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÑˆÐ°Ð³.\n\n"
            f"**{q}**"
        )
        if not picked_title_c:
            picked_title_c = "Ð ÑƒÐ±Ñ€Ð¸ÐºÐ° ÐºÐ°Ð½Ð°Ð»Ð° (Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð´Ð»Ñ ÑÐ°Ð¼Ð¾Ð½Ð°Ð±Ð»ÑŽÐ´ÐµÐ½Ð¸Ñ)"
        if not summary_c:
            summary_c = "Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚: Ð½Ð°Ð±Ð»ÑŽÐ´ÐµÐ½Ð¸Ðµ, Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ¸Ð¹ ÑˆÐ°Ð³, Ð±ÐµÐ· Ð´Ð°Ð²Ð»ÐµÐ½Ð¸Ñ."
    else:
        essence_lines = []
        if picked_title_c:
            essence_lines.append(f"ÐœÐ°Ñ‚ÐµÑ€Ð¸Ð°Ð»: {picked_title_c}")
        if summary_c:
            essence_lines.append(f"ÐšÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾: {summary_c}")
        essence = "\n".join(essence_lines).strip() or "ÐšÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾ Ð¸ Ð¿Ð¾ Ð´ÐµÐ»Ñƒ Ð¾ Ñ€Ð°Ð·Ð²Ð¸Ñ‚Ð¸Ð¸ Ñ€ÐµÑ‡Ð¸."

    # --- Ð§Ñ‚Ð¾ ÑÑ‚Ð¾ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ Ð´Ð»Ñ Ð²Ð°Ñ (2â€“3 Ð¿ÑƒÐ½ÐºÑ‚Ð°)
    meaning: List[str]
    if rf == "bilingual_parents":
        meaning = [
            "Ð¡Ð¼ÐµÑˆÐ¸Ð²Ð°Ð½Ð¸Ðµ ÑÐ·Ñ‹ÐºÐ¾Ð² Ð¸ â€œÐ²ÑÑ‚Ð°Ð²ÐºÐ¸â€ ÑÐ»Ð¾Ð² Ð²Ñ‚Ð¾Ñ€Ð¾Ð³Ð¾ ÑÐ·Ñ‹ÐºÐ° Ñ‡Ð°ÑÑ‚Ð¾ Ð±Ñ‹Ð²Ð°ÑŽÑ‚ Ñ‡Ð°ÑÑ‚ÑŒÑŽ Ð½Ð¾Ñ€Ð¼Ñ‹ Ð² Ð±Ð¸Ð»Ð¸Ð½Ð³Ð²Ð¸Ð·Ð¼Ðµ.",
            "Ð—Ð°Ð¿Ñ€ÐµÑ‚Ñ‹ Ð¸ Ð´Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾ ÑÐ½Ð¸Ð¶Ð°ÑŽÑ‚ Ð¼Ð¾Ñ‚Ð¸Ð²Ð°Ñ†Ð¸ÑŽ Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ñ‚ÑŒ â€” Ð»ÑƒÑ‡ÑˆÐµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°Ñ‚ÑŒ Ñ€ÑƒÑÑÐºÐ¸Ð¹ Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð½Ð¾ Ð¸ ÑÐ¿Ð¾ÐºÐ¾Ð¹Ð½Ð¾.",
            "Ð’Ð°Ð¶Ð½ÐµÐµ ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ Ð½Ð° Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð¸ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÑƒ, Ð° Ð½Ðµ Ð½Ð° Ð¸Ð´ÐµÐ°Ð»ÑŒÐ½ÑƒÑŽ â€œÑ‡Ð¸ÑÑ‚Ð¾Ñ‚Ñƒâ€ ÑÐ·Ñ‹ÐºÐ° Ð² ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð¼Ð¾Ð¼ÐµÐ½Ñ‚.",
        ]
    elif rf == "exercise_steps":
        meaning = [
            "ÐšÐ¾Ñ€Ð¾Ñ‚ÐºÐ°Ñ Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð½Ð°Ñ Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÐ° ÑÑ„Ñ„ÐµÐºÑ‚Ð¸Ð²Ð½ÐµÐµ Ñ€ÐµÐ´ÐºÐ¸Ñ… â€œÐ´Ð»Ð¸Ð½Ð½Ñ‹Ñ…â€ Ð·Ð°Ð½ÑÑ‚Ð¸Ð¹.",
            "Ð—ÐµÑ€ÐºÐ°Ð»Ð¾ Ð¸ Ð¸Ð³Ñ€Ð° Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÑŽÑ‚ ÑƒÐ´ÐµÑ€Ð¶Ð°Ñ‚ÑŒ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð¸ ÑÐ´ÐµÐ»Ð°Ñ‚ÑŒ ÑƒÐ¿Ñ€Ð°Ð¶Ð½ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¸Ð²Ñ‹Ñ‡ÐºÐ¾Ð¹.",
            "Ð•ÑÐ»Ð¸ Ñ€ÐµÐ±Ñ‘Ð½Ð¾Ðº ÑƒÑÑ‚Ð°Ð» â€” Ð»ÑƒÑ‡ÑˆÐµ Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒÑÑ Ñ€Ð°Ð½ÑŒÑˆÐµ, Ñ‡ÐµÐ¼ Ð·Ð°ÐºÑ€ÐµÐ¿Ð¸Ñ‚ÑŒ ÑÐ¾Ð¿Ñ€Ð¾Ñ‚Ð¸Ð²Ð»ÐµÐ½Ð¸Ðµ.",
        ]
    elif rf == "myth_fact":
        meaning = [
            "ÐŸÐ¾Ð»ÐµÐ·Ð½Ð¾ Ð¾Ñ‚Ð´ÐµÐ»ÑÑ‚ÑŒ Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ðµ Ð¼Ð¸Ñ„Ñ‹ Ð¾Ñ‚ Ñ‚Ð¾Ð³Ð¾, Ñ‡Ñ‚Ð¾ Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ Ð½Ð°Ð±Ð»ÑŽÐ´Ð°ÐµÑ‚ÑÑ Ð² Ñ€Ð°Ð·Ð²Ð¸Ñ‚Ð¸Ð¸ Ñ€ÐµÑ‡Ð¸.",
            "ÐžÐ±Ñ‹Ñ‡Ð½Ð¾ Ð²Ð°Ð¶Ð½ÐµÐµ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ, ÐºÐ¾Ð¼Ð¼ÑƒÐ½Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð¸ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ°, Ñ‡ÐµÐ¼ ÐµÐ´Ð¸Ð½Ð¸Ñ‡Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸.",
            "Ð•ÑÐ»Ð¸ Ñ‚Ñ€ÐµÐ²Ð¾Ð¶Ð½Ð¾ â€” Ð»ÑƒÑ‡ÑˆÐµ ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ Ð½Ð° ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸ÑŽ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½Ð¾, Ð° Ð½Ðµ Ð¿Ð¾ Ð¾Ð´Ð½Ð¾Ð¼Ñƒ ÑÐ¸Ð¼Ð¿Ñ‚Ð¾Ð¼Ñƒ.",
        ]
    elif rf == "age_norms":
        meaning = [
            "Ð’Ð¾Ð·Ñ€Ð°ÑÑ‚Ð½Ñ‹Ðµ Ð½Ð¾Ñ€Ð¼Ñ‹ â€” Ð¾Ñ€Ð¸ÐµÐ½Ñ‚Ð¸Ñ€, Ð° Ð½Ðµ â€œÑÐºÐ·Ð°Ð¼ÐµÐ½â€: Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ñ‹ Ð½Ð¾Ñ€Ð¼Ñ‹ Ð±Ñ‹Ð²Ð°ÑŽÑ‚ ÑˆÐ¸Ñ€Ð¾ÐºÐ¸Ð¼Ð¸.",
            "Ð“Ð»Ð°Ð²Ð½Ð¾Ðµ â€” Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ°: Ñ€Ð°ÑÑ‚Ñ‘Ñ‚ Ð»Ð¸ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð¸ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ñ‚Ð¸Ð²Ð° Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ, Ð¿Ð¾ÑÐ²Ð»ÑÑŽÑ‚ÑÑ Ð»Ð¸ Ð½Ð¾Ð²Ñ‹Ðµ ÑÐ»Ð¾Ð²Ð°/Ñ„Ñ€Ð°Ð·Ñ‹.",
            "Ð£Ð´Ð¾Ð±Ð½ÐµÐµ Ð¾Ð±ÑÑƒÐ¶Ð´Ð°Ñ‚ÑŒ ÑÐ¾Ð¼Ð½ÐµÐ½Ð¸Ñ Ð¿Ð¾ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¼ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð°Ð¼, Ð° Ð½Ðµ â€œÐ¿Ð¾ Ð¾Ñ‰ÑƒÑ‰ÐµÐ½Ð¸ÑÐ¼â€.",
        ]
    elif rf in ("pro_friendly","case_digest"):
        if aud == "parents":
            meaning = [
                "ÐÐ¸Ð¶Ðµ â€” Ð¸Ð´ÐµÑ, ÐºÐ°Ðº Ð¿Ñ€ÐµÐ²Ñ€Ð°Ñ‚Ð¸Ñ‚ÑŒ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð» Ð² Ð¿Ð¾Ð½ÑÑ‚Ð½Ñ‹Ð¹ Ð´Ð¾Ð¼Ð°ÑˆÐ½Ð¸Ð¹ ÑˆÐ°Ð³ Ð±ÐµÐ· Ð¿ÐµÑ€ÐµÐ³Ñ€ÑƒÐ·Ð°.",
                "Ð•ÑÐ»Ð¸ Ñ€ÐµÐ±Ñ‘Ð½ÐºÑƒ ÑÐ»Ð¾Ð¶Ð½Ð¾ â€” Ð½Ð°Ñ‡Ð¸Ð½Ð°Ð¹Ñ‚Ðµ Ñ Ð¼Ð°Ð»Ð¾Ð³Ð¾ Ð¸ Ñ„Ð¸ÐºÑÐ¸Ñ€ÑƒÐ¹Ñ‚Ðµ Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ¸Ð¹ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ.",
                "Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð½Ð¾ÑÑ‚ÑŒ Ð²Ð°Ð¶Ð½ÐµÐµ Ð¸Ð´ÐµÐ°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ.",
            ]
        else:
            meaning = [
                "ÐŸÐµÑ€ÐµÐ½Ð¾ÑÐ¸Ñ‚Ðµ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð» Ð² Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÑƒ: Ñ†ÐµÐ»ÑŒ â†’ ÐºÑ€Ð¸Ñ‚ÐµÑ€Ð¸Ð¹ â†’ ÑˆÐ°Ð³Ð¸ â†’ ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒ.",
                "Ð”Ð»Ñ Ð¾Ð½Ð»Ð°Ð¹Ð½ Ð¾ÑÐ¾Ð±ÐµÐ½Ð½Ð¾ Ð²Ð°Ð¶Ð½Ñ‹ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ, Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ðµ Ð”Ð— Ð¸ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ Ñ‡ÐµÐº-Ð»Ð¸ÑÑ‚.",
                "Ð£Ñ‡Ð¸Ñ‚Ñ‹Ð²Ð°Ð¹Ñ‚Ðµ Ð±Ð¸Ð»Ð¸Ð½Ð³Ð²Ð°Ð»ÑŒÐ½ÑƒÑŽ ÑÑ€ÐµÐ´Ñƒ Ð¸ Ð¿ÐµÑ€ÐµÐ½Ð¾Ñ Ð½Ð°Ð²Ñ‹ÐºÐ¾Ð² Ð¼ÐµÐ¶Ð´Ñƒ ÑÐ·Ñ‹ÐºÐ°Ð¼Ð¸.",
            ]
    else:
        meaning = [
            "Ð¡Ð°Ð¼Ñ‹Ð¹ Ð½Ð°Ð´Ñ‘Ð¶Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ Ð² Ñ€ÐµÑ‡Ð¸ â€” Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð½Ñ‹Ðµ Ð¼Ð°Ð»ÐµÐ½ÑŒÐºÐ¸Ðµ ÑˆÐ°Ð³Ð¸, Ð° Ð½Ðµ Ñ€Ð°Ð·Ð¾Ð²Ñ‹Ðµ â€œÑ€Ñ‹Ð²ÐºÐ¸â€.",
            "ÐšÐ¾Ð¼Ð¼ÑƒÐ½Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð²Ð°Ð¶Ð½ÐµÐµ Ð¸Ð´ÐµÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»ÑÑ†Ð¸Ð¸: ÑÐ½Ð°Ñ‡Ð°Ð»Ð° ÑÐ¼Ñ‹ÑÐ» Ð¸ Ð¶ÐµÐ»Ð°Ð½Ð¸Ðµ Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ñ‚ÑŒ, Ð¿Ð¾Ñ‚Ð¾Ð¼ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ.",
            "Ð›ÑƒÑ‡ÑˆÐµ Ð¾Ð¿Ð¸Ñ€Ð°Ñ‚ÑŒÑÑ Ð½Ð° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐµÐ½Ð½Ñ‹Ðµ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ Ð¸ Ð½Ð°Ð±Ð»ÑŽÐ´Ð°Ñ‚ÑŒ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÑƒ 2â€“4 Ð½ÐµÐ´ÐµÐ»Ð¸.",
        ]

    # --- ÐŸÑ€Ð°ÐºÑ‚Ð¸ÐºÐ° 5â€“7 Ð¼Ð¸Ð½ÑƒÑ‚
    practice: List[str]
    if rf == "exercise_steps":
        practice = [
            "ÐŸÐµÑ€ÐµÐ´ Ð·ÐµÑ€ÐºÐ°Ð»Ð¾Ð¼: Â«Ð›Ð¾Ð¿Ð°Ñ‚Ð¾Ñ‡ÐºÐ°Â» â€” 5 Ñ€Ð°Ð· Ð¿Ð¾ 5 ÑÐµÐºÑƒÐ½Ð´.",
            "Â«Ð§Ð°ÑÐ¸ÐºÐ¸Â» â€” 10 Ð¿Ð»Ð°Ð²Ð½Ñ‹Ñ… Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸Ð¹ Ð²Ð¿Ñ€Ð°Ð²Ð¾-Ð²Ð»ÐµÐ²Ð¾.",
            "1 Ð¼Ð¸Ð½ÑƒÑ‚Ð°: Ð´ÑƒÐµÐ¼ Ð½Ð° Ð²Ð°Ñ‚Ð½Ñ‹Ð¹ ÑˆÐ°Ñ€Ð¸Ðº/Ð¼Ñ‹Ð»ÑŒÐ½Ñ‹Ðµ Ð¿ÑƒÐ·Ñ‹Ñ€Ð¸ (Ð² Ð¸Ð³Ñ€Ðµ).",
            "Ð’ ÐºÐ¾Ð½Ñ†Ðµ â€” ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ°Ñ Ð¿Ð¾Ñ…Ð²Ð°Ð»Ð° Ð·Ð° Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ¸, Ð±ÐµÐ· â€œÐ¿ÐµÑ€ÐµÐ´ÐµÐ»Ñ‹Ð²Ð°Ð¹â€.",
        ]
    elif rf == "bilingual_parents":
        practice = [
            "Ð˜Ð³Ñ€Ð° â€œÐ”Ð²Ð° Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ð°â€: Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð¸Ñ‚Ðµ Ñ„Ñ€Ð°Ð·Ñƒ Ñ€ÐµÐ±Ñ‘Ð½ÐºÐ° Ð¿Ð¾-Ñ€ÑƒÑÑÐºÐ¸ (ÑÐ¿Ð¾ÐºÐ¾Ð¹Ð½Ð¾, Ð±ÐµÐ· Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ð¹), Ð·Ð°Ñ‚ÐµÐ¼ ÑÐ¿Ñ€Ð¾ÑÐ¸Ñ‚Ðµ Â«ÐºÐ°Ðº Ð¿Ð¾-Ñ€ÑƒÑÑÐºÐ¸?Â».",
            "5 Ð¼Ð¸Ð½ÑƒÑ‚ â€œÐ¾ÑÑ‚Ñ€Ð¾Ð²ÐºÐ° Ñ€ÑƒÑÑÐºÐ¾Ð³Ð¾â€: ÐºÐ½Ð¸Ð¶ÐºÐ°/ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐ¸/ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ð¹ Ð´Ð¾Ð¼Ð°.",
            "Ð’ ÐºÐ¾Ð½Ñ†Ðµ Ð´Ð½Ñ: Ñ€ÐµÐ±Ñ‘Ð½Ð¾Ðº Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÐµÑ‚ 3 Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚Ð° Ð¸ Ð½Ð°Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ð¸Ñ… Ð¿Ð¾-Ñ€ÑƒÑÑÐºÐ¸ (Ð¼Ð¾Ð¶Ð½Ð¾ Ñ Ð¿Ð¾Ð´ÑÐºÐ°Ð·ÐºÐ¾Ð¹).",
        ]
    elif rf == "question_week":
        practice = [
            "Ð—Ð°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ 3 Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð° Ñ„Ñ€Ð°Ð· Ñ€ÐµÐ±Ñ‘Ð½ÐºÐ° (ÐºÐ°Ðº ÐµÑÑ‚ÑŒ) Ð¸ ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸Ð¸, Ð³Ð´Ðµ Ð¾Ð½Ð¸ Ð¿Ñ€Ð¾Ð·Ð²ÑƒÑ‡Ð°Ð»Ð¸.",
            "ÐžÑ‚Ð¼ÐµÑ‚ÑŒÑ‚Ðµ: Ð¿Ð¾Ð½Ð¸Ð¼Ð°ÐµÑ‚ Ð»Ð¸ Ñ€ÐµÐ±Ñ‘Ð½Ð¾Ðº Ð¿Ñ€Ð¾ÑÑŒÐ±Ñ‹ Ð±ÐµÐ· Ð¶ÐµÑÑ‚Ð¾Ð² (2â€“3 Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð°).",
            "Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ 1 Ð¼Ð¸Ð½Ð¸-Ð¸Ð³Ñ€Ñƒ Ð½Ð° Ñ€ÐµÑ‡ÑŒ Ð½Ð° 5 Ð¼Ð¸Ð½ÑƒÑ‚ (ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐ¸/Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚Ð°/Ð¿ÑƒÐ·Ñ‹Ñ€Ð¸).",
        ]
    elif rf == "age_norms":
        practice = [
            "5 Ð¼Ð¸Ð½ÑƒÑ‚ â€œÐ¾Ð¿Ð¸ÑÐ°Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ñ€ÐµÑ‡Ð¸â€: Ð²Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚ Ð¸ Ð¿Ñ€Ð¾Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ñ‚Ðµ (Ñ†Ð²ÐµÑ‚/Ñ„Ð¾Ñ€Ð¼Ð°/Ð½Ð°Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ/Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ðµ).",
            "Ð˜Ð³Ñ€Ð° â€œÐšÑ‚Ð¾ Ñ‡Ñ‚Ð¾ Ð´ÐµÐ»Ð°ÐµÑ‚?â€: 10 Ð³Ð»Ð°Ð³Ð¾Ð»Ð¾Ð² Ð¿Ð¾ ÐºÐ°Ñ€Ñ‚Ð¸Ð½ÐºÐ°Ð¼ (Ð¿Ñ€Ñ‹Ð³Ð°ÐµÑ‚, Ñ€Ð¸ÑÑƒÐµÑ‚, Ð¼Ð¾ÐµÑ‚â€¦).",
            "Ð•ÑÐ»Ð¸ Ñ€ÐµÐ±Ñ‘Ð½Ð¾Ðº Ð±Ð¸Ð»Ð¸Ð½Ð³Ð² â€” Ð´Ð°Ð¹Ñ‚Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ, Ð·Ð°Ñ‚ÐµÐ¼ Ð¼ÑÐ³ÐºÐ¾ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð¸Ñ‚Ðµ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¿Ð¾-Ñ€ÑƒÑÑÐºÐ¸.",
        ]
    elif rf == "myth_fact":
        practice = [
            "Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ 1 ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸ÑŽ Ð´Ð»Ñ ÑÐ¿Ð¾ÐºÐ¾Ð¹Ð½Ð¾Ð³Ð¾ â€œÐ¼Ð¾Ð´ÐµÐ»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñâ€: Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð¸Ñ‚Ðµ Ñ„Ñ€Ð°Ð·Ñƒ Ñ€ÐµÐ±Ñ‘Ð½ÐºÐ° Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾, Ð±ÐµÐ· Ð¾Ñ†ÐµÐ½ÐºÐ¸.",
            "5 Ð¼Ð¸Ð½ÑƒÑ‚ Ð¸Ð³Ñ€Ñ‹ Ð½Ð° ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ (ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸: ÐµÐ´Ð°/Ð¾Ð´ÐµÐ¶Ð´Ð°/Ð¸Ð³Ñ€ÑƒÑˆÐºÐ¸).",
            "Ð’ ÐºÐ¾Ð½Ñ†Ðµ â€” Ð¾Ð´Ð¸Ð½ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ñ‹Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ: Â«Ð§Ñ‚Ð¾ Ð±Ñ‹Ð»Ð¾ ÑÐ°Ð¼Ñ‹Ð¼ Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ñ‹Ð¼?Â»",
        ]
    elif rf in ("pro_friendly","case_digest") and aud != "parents":
        practice = [
            "Ð¡Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ Ñ†ÐµÐ»ÑŒ Ð½Ð° 2 Ð½ÐµÐ´ÐµÐ»Ð¸ (1â€“2 Ð¸Ð·Ð¼ÐµÑ€Ð¸Ð¼Ñ‹Ñ… ÐºÑ€Ð¸Ñ‚ÐµÑ€Ð¸Ñ).",
            "ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÑŒÑ‚Ðµ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸ÑŽ 1 ÑƒÐ¿Ñ€Ð°Ð¶Ð½ÐµÐ½Ð¸Ñ + Ñ‡ÐµÐº-Ð»Ð¸ÑÑ‚ Ð´Ð»Ñ Ñ€Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÐµÐ¹ (Ð´Ð¾ 6 Ð¿ÑƒÐ½ÐºÑ‚Ð¾Ð²).",
            "ÐŸÑ€Ð¾Ð´ÑƒÐ¼Ð°Ð¹Ñ‚Ðµ Ð¿ÐµÑ€ÐµÐ½Ð¾Ñ Ð² Ð±Ð¸Ð»Ð¸Ð½Ð³Ð²Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÑÑ€ÐµÐ´Ðµ: Ð³Ð´Ðµ Ð·Ð°ÐºÑ€ÐµÐ¿Ð»ÑÑ‚ÑŒ Ñ€ÑƒÑÑÐºÐ¸Ð¹ ÐµÐ¶ÐµÐ´Ð½ÐµÐ²Ð½Ð¾ 5â€“10 Ð¼Ð¸Ð½ÑƒÑ‚.",
        ]
    else:
        practice = [
            "5 Ð¼Ð¸Ð½ÑƒÑ‚ Ð°Ñ€Ñ‚Ð¸ÐºÑƒÐ»ÑÑ†Ð¸Ð¾Ð½Ð½Ð¾Ð¹ Ð³Ð¸Ð¼Ð½Ð°ÑÑ‚Ð¸ÐºÐ¸ (Ð² Ð¸Ð³Ñ€Ðµ, Ð¿ÐµÑ€ÐµÐ´ Ð·ÐµÑ€ÐºÐ°Ð»Ð¾Ð¼).",
            "5 Ð¼Ð¸Ð½ÑƒÑ‚ â€œÑÐ»Ð¾Ð²Ð°Ñ€Ð½Ð¾Ð¹ Ð¸Ð³Ñ€Ñ‹â€: ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸/Ð¿Ñ€Ð¾Ñ‚Ð¸Ð²Ð¾Ð¿Ð¾Ð»Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸/Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚Ð¾Ð².",
            "1 Ð¼Ð¸Ð½ÑƒÑ‚Ð° Ð´Ñ‹Ñ…Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¸Ð³Ñ€Ñ‹ (Ð¿ÑƒÐ·Ñ‹Ñ€Ð¸/Ð²Ð°Ñ‚Ð½Ñ‹Ð¹ ÑˆÐ°Ñ€Ð¸Ðº/Ð´ÑƒÐµÐ¼ Ð½Ð° Ð¿ÐµÑ€Ñ‹ÑˆÐºÐ¾).",
        ]

    # --- ÐÐ¾Ñ€Ð¼Ð° / ÐºÐ¾Ð³Ð´Ð° Ð½ÑƒÐ¶ÐµÐ½ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸ÑÑ‚
    if rf in ("pro_friendly","case_digest") and aud != "parents":
        norm_lines = [
            "âœ… ÐÐ¾Ñ€Ð¼Ð°: ÐµÑÑ‚ÑŒ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚, Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¹, Ð¿Ð¾ÑÑ‚ÐµÐ¿ÐµÐ½Ð½Ð°Ñ Ð´Ð¸Ð½Ð°Ð¼Ð¸ÐºÐ° Ð¿Ð¾ Ñ†ÐµÐ»ÑÐ¼.",
            "âš ï¸ ÐžÐ±ÑÑƒÐ´Ð¸Ñ‚ÑŒ ÑÐ¾ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸ÑÑ‚Ð¾Ð¼: Ð²Ñ‹Ñ€Ð°Ð¶ÐµÐ½Ð½Ð°Ñ Ñ€ÐµÐ³Ñ€ÐµÑÑÐ¸Ñ Ð½Ð°Ð²Ñ‹ÐºÐ¾Ð² Ð¸Ð»Ð¸ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²Ð¸Ðµ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ° Ð¿Ñ€Ð¸ Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð½Ð¾Ð¹ Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÐµ 4â€“6 Ð½ÐµÐ´ÐµÐ»ÑŒ.",
        ]
    else:
        norm_lines = [
            "âœ… ÐÐ¾Ñ€Ð¼Ð°: Ñ€ÐµÐ±Ñ‘Ð½Ð¾Ðº Ð¿Ð¾Ð½Ð¸Ð¼Ð°ÐµÑ‚ Ð¾Ð±Ñ€Ð°Ñ‰Ñ‘Ð½Ð½ÑƒÑŽ Ñ€ÐµÑ‡ÑŒ, Ð¾Ð±Ñ‰Ð°ÐµÑ‚ÑÑ (Ð¶ÐµÑÑ‚Ð°Ð¼Ð¸/ÑÐ»Ð¾Ð²Ð°Ð¼Ð¸), Ð¸ ÐµÑÑ‚ÑŒ Ð¿Ð¾ÑÑ‚ÐµÐ¿ÐµÐ½Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ Ð¿Ð¾ Ð½ÐµÐ´ÐµÐ»ÑÐ¼.",
            "âš ï¸ ÐžÐ±ÑÑƒÐ´Ð¸Ñ‚ÑŒ ÑÐ¾ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸ÑÑ‚Ð¾Ð¼: ÐµÑÐ»Ð¸ Ñ€ÐµÐ±Ñ‘Ð½Ð¾Ðº Ñ‡Ð°ÑÑ‚Ð¾ Ð½Ðµ Ð¿Ð¾Ð½Ð¸Ð¼Ð°ÐµÑ‚ Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ðµ Ð¿Ñ€Ð¾ÑÑŒÐ±Ñ‹, Ñ€ÐµÐ·ÐºÐ¾ â€œÑ‚ÐµÑ€ÑÐµÑ‚â€ Ð½Ð°Ð²Ñ‹ÐºÐ¸ Ð¸Ð»Ð¸ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ° Ð½ÐµÑ‚ Ð¿Ñ€Ð¸ Ñ€ÐµÐ³ÑƒÐ»ÑÑ€Ð½Ð¾Ð¹ Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÐµ 4â€“6 Ð½ÐµÐ´ÐµÐ»ÑŒ.",
        ]

    factcheck = picked.get("fact_check") or ""
    stype = picked.get("source_type") or source_type_label_from_factcheck(factcheck)

    ok, q_reason = _quality_gate(rf, aud, link, essence, meaning, practice, norm_lines)
    meta = {
        "ok": ok,
        "reason": q_reason,
        "rubric_format": rf,
        "audience": aud,
        "source_type": stype,
    }
    if not ok:
        return "", meta

    parts: List[str] = []
    parts.append(f"**{rubric_title} {title_suffix}**")
    parts.append("")
    parts.append("**Ð¡ÑƒÑ‚ÑŒ**")
    parts.append(essence)
    parts.append("")
    parts.append("**Ð§Ñ‚Ð¾ ÑÑ‚Ð¾ Ð·Ð½Ð°Ñ‡Ð¸Ñ‚ Ð´Ð»Ñ Ð²Ð°Ñ**")
    parts.append(_bullets(meaning))
    parts.append("")
    parts.append("**ÐŸÑ€Ð°ÐºÑ‚Ð¸ÐºÐ° Ð½Ð° ÑÐµÐ³Ð¾Ð´Ð½Ñ (5â€“7 Ð¼Ð¸Ð½ÑƒÑ‚)**")
    parts.append(_numbered(practice))
    parts.append("")
    parts.append("**ÐÐ¾Ñ€Ð¼Ð° / ÐºÐ¾Ð³Ð´Ð° Ð½ÑƒÐ¶ÐµÐ½ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸ÑÑ‚**")
    parts.append("\n".join(norm_lines))
    parts.append("")
    parts.append("**Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº**")
    parts.append(f"ðŸ”— {link}")
    parts.append(f"Ð¢Ð¸Ð¿: {stype}")

    if disclaimer:
        parts.append("")
        parts.append(f"_{disclaimer}_")
    if tags (:= tags):
        parts.append("")
        parts.append(tags)

    raw_text = "\n".join(parts).strip()

    final_text = rewrite_if_enabled(raw_text, aud)

    # final sanity check: rewrite must preserve structure
    if not _has_required_headings(final_text):
        meta["ok"] = False
        meta["reason"] = "quality_gate:rewrite_broke_structure"
        return "", meta

    return final_text, meta

# ---------------------------
# Card rendering (v1.4)
# ---------------------------

def _load_font(size: int) -> ImageFont.FreeTypeFont:
    ttf = FONTS_DIR/"DejaVuSans.ttf"
    if ttf.exists():
        return ImageFont.truetype(str(ttf), size=size)
    return ImageFont.load_default()

def _hex_to_rgb(h: str) -> Tuple[int,int,int]:
    h = (h or "").strip().lstrip("#")
    if len(h)==3:
        h = "".join([c+c for c in h])
    if len(h)!=6:
        return (74,144,226)
    return tuple(int(h[i:i+2],16) for i in (0,2,4))

def render_image_card(rubric_title: str, subtitle: str, branding: Dict[str,Any]) -> Path:
    """
    Visual themes (switch in config/rubrics.yml -> branding.card_theme):
      - minimal: clean neutral, subtle waves
      - kids: softer palette, playful dots
      - scientific: stricter palette, grid accents
    """
    theme = (branding or {}).get("card_theme","minimal") or "minimal"
    theme = str(theme).strip().lower()

    W,H = 1280,720
    accent = _hex_to_rgb((branding or {}).get("card_accent","#4A90E2"))

    # Theme palettes
    if theme == "kids":
        bg_top = (252, 246, 255)
        bg_bottom = (240, 252, 255)
        panel_fill = (255,255,255)
        panel_outline = (236,230,244)
        title_color = (32, 36, 46)
        sub_color = (78, 86, 104)
        footer_color = (120, 126, 140)
        wave_alpha = 30
    elif theme == "scientific":
        bg_top = (245, 247, 250)
        bg_bottom = (232, 236, 244)
        panel_fill = (255,255,255)
        panel_outline = (220,226,235)
        title_color = (16, 20, 30)
        sub_color = (54, 62, 78)
        footer_color = (98, 104, 118)
        wave_alpha = 22
        if sum(accent) > 560:
            accent = (36, 79, 166)
    else:  # minimal
        bg_top = (245, 247, 250)
        bg_bottom = (235, 240, 246)
        panel_fill = (255,255,255)
        panel_outline = (235,238,242)
        title_color = (24, 32, 44)
        sub_color = (70, 78, 92)
        footer_color = (110, 118, 132)
        wave_alpha = 26

    img = Image.new("RGB",(W,H),bg_top)
    draw = ImageDraw.Draw(img)

    # gradient background
    for y in range(H):
        t = y/(H-1)
        r = int(bg_top[0] + (bg_bottom[0]-bg_top[0])*t)
        g = int(bg_top[1] + (bg_bottom[1]-bg_top[1])*t)
        b = int(bg_top[2] + (bg_bottom[2]-bg_top[2])*t)
        draw.line([(0,y),(W,y)], fill=(r,g,b))

    # accents layer
    layer = Image.new("RGBA",(W,H),(0,0,0,0))
    ld = ImageDraw.Draw(layer)

    if theme in ("minimal","scientific"):
        for i in range(3):
            y0 = 440 + i*55
            pts=[]
            for x in range(0,W+1,40):
                yy = y0 + int(12*math.sin((x/140.0) + i))
                pts.append((x,yy))
            ld.line(pts, fill=(*accent, wave_alpha), width=6 if theme=="minimal" else 5)

        if theme == "scientific":
            gx0, gy0, gx1, gy1 = 760, 60, 1240, 300
            step = 34
            grid_col = (accent[0], accent[1], accent[2], 16)
            for x in range(gx0, gx1, step):
                ld.line([(x,gy0),(x,gy1)], fill=grid_col, width=2)
            for y in range(gy0, gy1, step):
                ld.line([(gx0,y),(gx1,y)], fill=grid_col, width=2)

    elif theme == "kids":
        seed = int(hashlib.sha1((rubric_title or "").encode("utf-8")).hexdigest()[:8], 16)
        rng = random.Random(seed)
        dot_col = (accent[0], accent[1], accent[2], 22)
        for _ in range(120):
            x = rng.randint(60, W-60)
            y = rng.randint(60, H-60)
            rr = rng.randint(3, 9)
            ld.ellipse([x-rr,y-rr,x+rr,y+rr], fill=dot_col)
        for cx,cy,rr in [(220,160,110),(1120,520,140)]:
            ld.ellipse([cx-rr,cy-rr,cx+rr,cy+rr], fill=(accent[0],accent[1],accent[2],18))

    img = Image.alpha_composite(img.convert("RGBA"), layer).convert("RGB")
    draw = ImageDraw.Draw(img)

    # panel + shadow
    panel = (70,90,W-70,H-110)
    shadow = Image.new("RGBA",(W,H),(0,0,0,0))
    sd = ImageDraw.Draw(shadow)
    sd.rounded_rectangle([panel[0]+6,panel[1]+10,panel[2]+6,panel[3]+10], radius=28, fill=(0,0,0,60))
    shadow = shadow.filter(ImageFilter.GaussianBlur(10))
    img = Image.alpha_composite(img.convert("RGBA"), shadow).convert("RGB")
    draw = ImageDraw.Draw(img)

    draw.rounded_rectangle(panel, radius=28, fill=panel_fill, outline=panel_outline, width=2)

    # accent bar
    ax = panel[0]+28
    ay = panel[1]+28
    draw.rounded_rectangle([ax, ay, ax+10, panel[3]-28], radius=6, fill=accent)

    # typography
    f_title = _load_font(56 if theme!="scientific" else 54)
    f_sub = _load_font(32 if theme!="scientific" else 30)
    f_small = _load_font(24)

    x_text = ax+28
    y_text = panel[1]+44
    max_w = panel[2]-x_text-28

    def wrap(text: str, font: ImageFont.ImageFont, max_width: int) -> List[str]:
        words = (text or "").split()
        if not words:
            return []
        lines=[]; cur=[]
        for w in words:
            test = " ".join(cur+[w])
            if draw.textlength(test, font=font) <= max_width:
                cur.append(w)
            else:
                if cur:
                    lines.append(" ".join(cur))
                cur=[w]
        if cur:
            lines.append(" ".join(cur))
        return lines

    for ln in wrap(rubric_title, f_title, max_w)[:3]:
        draw.text((x_text, y_text), ln, fill=title_color, font=f_title)
        y_text += 68

    y_text += 12
    for ln in wrap(subtitle, f_sub, max_w)[:3]:
        draw.text((x_text, y_text), ln, fill=sub_color, font=f_sub)
        y_text += 44

    footer = (branding or {}).get("card_footer","")
    if footer:
        draw.text((panel[0]+28, panel[3]-48), footer, fill=footer_color, font=f_small)

    out = STATE_DIR/f"card_{sha1(theme+rubric_title+subtitle)[:10]}.png"
    img.save(out)
    return out

# ---------------------------
# Telegram helpers + stats + selection
# ---------------------------

def tg_request(method: str, data: Dict[str,Any], files: Optional[Dict[str,Any]]=None) -> Dict[str,Any]:
    if not TELEGRAM_BOT_TOKEN:
        raise RuntimeError("TELEGRAM_BOT_TOKEN is missing.")
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/{method}"
    r = requests.post(url, data=data, files=files, timeout=30)

    if not r.ok:
        desc = ""
        try:
            j = r.json()
            desc = j.get("description","") or str(j)
        except Exception:
            desc = (r.text or "")[:800]
        print(f"[TG] {method} failed: {r.status_code} :: {desc[:800]}")
        r.raise_for_status()

    return r.json()

def send_photo(chat_id: str, photo_path: Path, caption: str) -> None:
    if not chat_id:
        raise RuntimeError("TELEGRAM_CHAT_ID is missing/empty.")
    cap = (caption or "").strip()

    # 1) Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Markdown
    try:
        with photo_path.open("rb") as f:
            tg_request(
                "sendPhoto",
                data={"chat_id": chat_id, "caption": cap[:1000], "parse_mode":"Markdown"},
                files={"photo": f}
            )
        return
    except requests.exceptions.HTTPError as e:
        resp_text = ""
        if getattr(e, "response", None) is not None:
            resp_text = (e.response.text or "")
        if "parse entities" in resp_text.lower() or "can't parse" in resp_text.lower():
            print("[WARN] Telegram Markdown parse error in caption; retry without parse_mode")
            cap_plain = markdown_to_plain(cap)
            with photo_path.open("rb") as f:
                tg_request(
                    "sendPhoto",
                    data={"chat_id": chat_id, "caption": cap_plain[:1000]},
                    files={"photo": f}
                )
            return
        raise

def send_message(chat_id: str, text: str) -> None:
    if not chat_id:
        raise RuntimeError("TELEGRAM_CHAT_ID is missing/empty.")
    t = (text or "").strip()

    # 1) Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Markdown
    try:
        tg_request("sendMessage", data={"chat_id": chat_id, "text": t[:4000], "parse_mode":"Markdown"})
        return
    except requests.exceptions.HTTPError as e:
        resp_text = ""
        if getattr(e, "response", None) is not None:
            resp_text = (e.response.text or "")
        if "parse entities" in resp_text.lower() or "can't parse" in resp_text.lower():
            print("[WARN] Telegram Markdown parse error in message; retry without parse_mode")
            tp = markdown_to_plain(t)
            tg_request("sendMessage", data={"chat_id": chat_id, "text": tp[:4000]})
            return
        raise

def load_weekly_stats() -> Dict[str,Any]:
    return load_state("stats_weekly.json", {})

def save_weekly_stats(stats: Dict[str,Any]) -> None:
    save_state("stats_weekly.json", stats)

def bump_weekly(stats: Dict[str,Any], week_key: str, field: str, amount: int = 1, reason: Optional[str]=None) -> None:
    wk = stats.get(week_key) or {"passed": 0, "rejected": 0, "reasons": {}}
    wk[field] = int(wk.get(field,0)) + amount
    if reason:
        rs = wk.get("reasons") or {}
        rs[reason] = int(rs.get(reason,0)) + amount
        wk["reasons"] = rs
    stats[week_key] = wk

def format_dashboard(stats: Dict[str,Any], week_key: str, title: str) -> str:
    wk = stats.get(week_key) or {"passed": 0, "rejected": 0, "reasons": {}}
    passed = int(wk.get("passed",0))
    rejected = int(wk.get("rejected",0))
    reasons = wk.get("reasons") or {}
    top = sorted(reasons.items(), key=lambda x: x[1], reverse=True)[:6]
    lines = [f"**{title} ({week_key})**",
             "",
             f"âœ… ÐŸÑ€Ð¾ÑˆÐ»Ð¾: {passed}",
             f"ðŸ—‚ï¸ Ð’ Ñ‡ÐµÑ€Ð½Ð¾Ð²Ð¸ÐºÐ¸/Ð¾Ñ‚ÑÐµÐ²: {rejected}",
             ""]
    if top:
        lines.append("ÐŸÑ€Ð¸Ñ‡Ð¸Ð½Ñ‹ Ð¾Ñ‚ÑÐµÐ²Ð° (Ñ‚Ð¾Ð¿):")
        for k,v in top:
            lines.append(f"â€¢ {k}: {v}")
    else:
        lines.append("ÐŸÑ€Ð¸Ñ‡Ð¸Ð½Ñ‹ Ð¾Ñ‚ÑÐµÐ²Ð°: Ð½ÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ….")
    lines.append("")
    lines.append("_ÐŸÑ€Ð¸Ð¼ÐµÑ‡Ð°Ð½Ð¸Ðµ: ÑÑ‚Ð¾ Ñ‚ÐµÑ…. ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð²/Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²._")
    return "\n".join(lines)

def handle_draft(pub_cfg: Dict[str,Any], entry: Dict[str,Any], stats: Dict[str,Any], week_key: str) -> None:
    mode = (pub_cfg.get("drafts_mode") or "skip").strip()
    drafts_chat_id = ""
    if mode == "post_to_drafts_chat":
        env_name = pub_cfg.get("drafts_chat_id_env") or "TELEGRAM_DRAFTS_CHAT_ID"
        drafts_chat_id = os.getenv(env_name,"").strip() or TELEGRAM_DRAFTS_CHAT_ID

    drafts = load_state("drafts.json", [])
    drafts.append(entry)
    save_state("drafts.json", drafts[-2000:])

    bump_weekly(stats, week_key, "rejected", 1, reason=str(entry.get("reason","unknown")))

    if mode == "post_to_drafts_chat" and drafts_chat_id:
        msg = ("**Ð§ÐµÑ€Ð½Ð¾Ð²Ð¸Ðº/Ð¿Ñ€Ð¾Ð¿ÑƒÑÐº**\n\n"
               f"ÐŸÑ€Ð¸Ñ‡Ð¸Ð½Ð°: {entry.get('reason')}\n"
               f"Ð ÑƒÐ±Ñ€Ð¸ÐºÐ°: {entry.get('rubric_title','')}\n"
               f"ÐÑƒÐ´Ð¸Ñ‚Ð¾Ñ€Ð¸Ñ: {entry.get('audience','')}\n"
               f"Ð—Ð°Ð³Ð¾Ð»Ð¾Ð²Ð¾Ðº: {entry.get('title')}\n"
               f"Ð¡ÑÑ‹Ð»ÐºÐ°: {entry.get('link')}\n")
        send_message(drafts_chat_id, msg)

def pick_item(
    items: List[Dict[str,str]],
    used_canon: set[str],
    used_titles: set[str],
    quality_cfg: Dict[str,Any]
) -> Tuple[Optional[Dict[str,str]], Optional[Dict[str,Any]]]:
    ranked=[]
    for it in items:
        t = norm_space(it.get("title",""))
        l = it.get("link","")
        if not l:
            continue
        s,_ = score_item(t or "(no title)", l, quality_cfg)
        if s>=0:
            ranked.append((s,it))
    ranked.sort(key=lambda x:x[0], reverse=True)

    for _,it in ranked[:22]:
        it = enrich_article(dict(it))
        canon = it.get("canonical") or it.get("link","")
        if not canon or canon in used_canon:
            continue

        raw_title = it.get("article_title") or it.get("title") or ""
        tkey = norm_title_key(raw_title)
        if tkey and tkey in used_titles:
            continue

        dom = safe_domain(canon)
        summ = it.get("article_summary") or it.get("summary") or ""
        ok, reason = is_scientific_or_methodical(dom, raw_title, summ, quality_cfg)
        if not ok:
            return None, {
                "ts": datetime.utcnow().replace(tzinfo=timezone.utc).isoformat().replace("+00:00","Z"),
                "reason": f"fact_check_failed:{reason}",
                "title": raw_title,
                "link": canon,
                "domain": dom,
            }

        it["picked_title"]=raw_title
        it["picked_summary"]=summ
        it["fact_check"]=reason
        it["source_type"]=source_type_label_from_factcheck(reason)
        return it, None
    return None, None

def run() -> None:
    rub_cfg = load_yaml(CFG_DIR/"rubrics.yml")
    channel_cfg = rub_cfg.get("channel",{})
    branding = rub_cfg.get("branding",{})
    tzname = channel_cfg.get("timezone","Asia/Nicosia")
    now = get_local_now(tzname)
    week_key = iso_week_key(now)

    sources, quality_cfg = load_sources()
    used_canon = set(load_state("used_canonical.json",[]))
    used_titles = set(load_state("used_titles.json",[]))

    pub_cfg = rub_cfg.get("publishing",{})
    max_posts = int(pub_cfg.get("max_posts_per_run",3))
    max_per_aud = int(pub_cfg.get("max_posts_per_audience_per_run",2))

    stats = load_weekly_stats()

    audiences_cfg = rub_cfg.get("audiences",{})
    if AUDIENCE=="both":
        aud_list=["parents","pros"]
    elif AUDIENCE in ("parents","pros"):
        aud_list=[AUDIENCE]
    else:
        aud_list=["parents"]

    posted=0
    for aud in aud_list:
        if posted>=max_posts:
            break
        aud_cfg = audiences_cfg.get(aud,{})
        title_suffix = aud_cfg.get("title_suffix","")
        rubrics = aud_cfg.get("rubrics",[]) or []
        aud_posted=0

        for rubric in rubrics:
            if posted>=max_posts or aud_posted>=max_per_aud:
                break
            if not is_due(rubric, now):
                continue

            if rubric.get("format") == "quality_dashboard":
                dash_title = pub_cfg.get("dashboard_title","Quality dashboard Ð½ÐµÐ´ÐµÐ»Ð¸")
                dashboard_text = format_dashboard(stats, week_key, dash_title)
                dash_chat = (pub_cfg.get("dashboard_chat") or "main").strip().lower()
                chat_id = TELEGRAM_CHAT_ID
                if dash_chat == "drafts" and TELEGRAM_DRAFTS_CHAT_ID:
                    chat_id = TELEGRAM_DRAFTS_CHAT_ID
                send_message(chat_id, dashboard_text)
                time.sleep(0.7)
                continue

            all_items=[]
            for sid in rubric.get("sources",[]):
                src = sources.get(sid)
                if not src:
                    continue
                try:
                    all_items.extend(fetch_source(src))
                except Exception as e:
                    print(f"[WARN] source {sid} failed: {e}")

            picked, draft = pick_item(all_items, used_canon, used_titles, quality_cfg)
            if draft:
                draft.update({"audience": aud, "rubric": rubric.get("id",""), "rubric_title": rubric.get("title","")})
                handle_draft(pub_cfg, draft, stats, week_key)
                continue
            if not picked:
                continue

            title = rubric.get("title","Ð ÑƒÐ±Ñ€Ð¸ÐºÐ°")
            text, meta = compose_post_v2(title, rubric.get("format",""), aud, channel_cfg, picked, title_suffix)

            if not meta.get("ok", False):
                draft_entry = {
                    "ts": datetime.utcnow().replace(tzinfo=timezone.utc).isoformat().replace("+00:00","Z"),
                    "reason": str(meta.get("reason","quality_gate_failed")),
                    "audience": aud,
                    "rubric": rubric.get("id",""),
                    "rubric_title": title,
                    "title": picked.get("picked_title") or picked.get("title") or "",
                    "link": picked.get("canonical") or picked.get("link") or "",
                    "domain": safe_domain(picked.get("canonical") or picked.get("link") or ""),
                    "source_type": meta.get("source_type",""),
                }
                handle_draft(pub_cfg, draft_entry, stats, week_key)
                continue

            subtitle = "ÐšÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾ Ð¸ Ð¿Ð¾ Ð´ÐµÐ»Ñƒ"
            summ = (picked.get("picked_summary") or "").strip()
            if summ:
                subtitle = summ[:110].rstrip(" .,:;â€”-") + "â€¦"

            card = render_image_card(title, subtitle, branding)

            # Telegram sometimes fails on Markdown entities in caption; fallback to plain text + sendMessage
            try:
                send_photo(TELEGRAM_CHAT_ID, card, text[:950])
            except Exception as e:
                print(f"[WARN] send_photo failed; fallback to send_message. Err: {e}")
                try:
                    send_message(TELEGRAM_CHAT_ID, markdown_to_plain(text)[:4000])
                except Exception as e2:
                    print(f"[ERROR] send_message fallback also failed: {e2}")
                    raise

            bump_weekly(stats, week_key, "passed", 1)

            canon = picked.get("canonical") or picked.get("link","")
            if canon: used_canon.add(canon)
            tkey = norm_title_key(picked.get("picked_title") or picked.get("title") or "")
            if tkey: used_titles.add(tkey)

            posted += 1
            aud_posted += 1
            time.sleep(1.2)

    save_state("used_canonical.json", sorted(list(used_canon))[-6000:])
    save_state("used_titles.json", sorted(list(used_titles))[-6000:])
    save_weekly_stats(stats)

    print(f"Done. Posted: {posted}. Audience: {AUDIENCE}. Rewrite: {REWRITE_PROVIDER}. Week: {week_key}")

if __name__=="__main__":
    run()
